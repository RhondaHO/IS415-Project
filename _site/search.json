[
  {
    "objectID": "aps/template.html",
    "href": "aps/template.html",
    "title": "Aps Template",
    "section": "",
    "text": "TODO Create a template that demonstrates the appearance, formatting, layout, and functionality of your format. Learn more about journal formats at https://quarto.org/docs/journals/."
  },
  {
    "objectID": "project_proposal.html",
    "href": "project_proposal.html",
    "title": "G1T7 Project Research Paper",
    "section": "",
    "text": "This project aims to simplify the process of understanding amenities and facilities in the vicinity of desired HDB locations. The proposed analytical Shiny app utilizes advanced spatial point pattern analysis techniques to provide a customized view of amenities that match HDB buyers’ preferences. The project objectives include visualizing an overview of HDB flats in Singapore along with relevant amenity locations, measuring spatial association and heterogeneity, estimating HDB location intensity, assessing HDB location distribution non-randomness, and analyzing the spatial distribution of HDB flats over a street network. By empowering HDB buyers with a user-friendly application, the project seeks to assist them in making more informed decisions about their purchases and feeling more confident about their selected residence."
  },
  {
    "objectID": "project_proposal.html#project-objectives",
    "href": "project_proposal.html#project-objectives",
    "title": "G1T7 Project Research Paper",
    "section": "Project Objectives",
    "text": "Project Objectives\nThe objective of this project is to develop an analytical application that empowers HDB buyers with the following capabilities:\n\nVisualise an overview of HDB flats in Singapore along with relevant amenity locations, such as shopping malls, bus stops, etc. that match their preferences.\nMeasure the level of spatial association and heterogeneity between HDB locations and surrounding amenities using Colocation Quotients (CLQs) analysis.\nUse Kernel Density Estimation (KDE) to estimate the intensity of HDB locations in different study areas of Singapore, such as Tampines and Bedok.\nAssess the non-randomness of HDB location distribution in selected study areas, such as Tampines and Bedok, using F-function and Ripley’s L-function.\nApply Network Constrained Spatial Point Patterns Analysis to analyze the spatial distribution of HDB flats over a street network.\n\nBy incorporating advanced spatial point pattern analysis techniques, we aim to simplify the process for HDB buyers by providing them with a user-friendly application that visualises amenities and facilities in their desired location. Our objective is to identify significant spatial patterns and trends that can assist buyers in making informed decisions about their HDB flat purchases and feel confident about their selected residence."
  },
  {
    "objectID": "project_proposal.html#data-sets",
    "href": "project_proposal.html#data-sets",
    "title": "G1T7 Project/Research Paper",
    "section": "Data Sets",
    "text": "Data Sets\nBelow is a table of the data sets we will be using for our project.\n\n\n\nName\nDescription\nFile Format\n\n\n\n\nHDB Resale Flat Prices\nProvides HDB addresses, blocks and street name | Data.gov.sg\n.csv\n\n\nSchool Directory and Information\nProvides a list of primary schools in Singapore | Data.gov.sg\n.csv\n\n\nShopping Malls\nProvides a list of shopping mall and its geometry in Singapore | Web scrapped shopping mall data in 2019 by Valery Lim\n.csv\n\n\nURA Master Plan 2019 Subzone Boundary\nProvides region boundary data | Referenced/taken from Prof Kam\nshp\n\n\nBus Stops\nProvides a list of bus stops and its geometry in Singapore | Datamall LTA\n.shp\n\n\nTrain Station\nProvides a list of MRT/LTR exits and its geometry in Singapore | Datamall LTA\n.shp\n\n\nSupermarkets\nProvides a list of supermarkets in Singapore | Data.gov.sg\n.geojson\n\n\nChildcare Centres\nProvides a list of names, addresses and relevant information for childcare centres in Singapore | Extracted via onemapAPI API Docs | Registration\n.rds\n\n\nEldercare Centres\nProvides a list of names, addresses and relevant information for eldercare centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nKindergartens\nProvides a list of names, addresses and relevant information for kindergartens in Singapore | Extracted via onemapAPI\n.rds\n\n\nHawker Centres\nProvides a list of names, addresses and relevant information for hawker centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nHealthier Hawker Centres\nProvides a list of names, addresses and relevant information for healthier hawker centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nNational Parks\nProvides a list of names, addresses and relevant information for national parks in Singapore | Extracted via onemapAPI\n.rds\n\n\nGyms\nProvides a list of names, addresses and relevant information for gyms in Singapore | Extracted via onemapAPI\n.rds\n\n\nRetail Pharmacies\nProvides a list of names, addresses and relevant information for retail pharmacies in Singapore | Extracted via onemapAPI\n.rds\n\n\nSingapore Police Force (SPF) Establishments\nProvides a list of names, addresses and relevant information for SPF establishments in Singapore | Extracted via onemapAPI\n.rds\n\n\nCarparks\nProvides a list of names, addresses and relevant information for carparks in Singapore | Extracted via onemapAPI\n.rds"
  },
  {
    "objectID": "project_proposal.html#literature-review",
    "href": "project_proposal.html#literature-review",
    "title": "G1T7 Project Research Paper",
    "section": "Literature Review",
    "text": "Literature Review\n\n1. MEPHAS: an interactive graphical user interface for medical and pharmaceutical statistical analysis with R and Shiny (Zhou et al, 2020)\nThis article highlights that the MEPHAS tool has been designed to support various statistical analyses, including probability, hypothesis testing, regression modeling, and dimensional analysis. Each design interface contains multiple tabs for different analysis methods, an input panel on the left, and an output panel on the right. MEPHAS also allows for data input, parameter configuration, and result output.\n\nHow does it link to our project?\nWe plan to follow and utilise a similar design of the MEPHAS tool, making it a useful template for structuring your HDB app. We will be including navigation bar/tab panels to allow users to switch between the different spatial point analysis tools and implementing a side panel to allow users customize the visualizations and a main panel to generate the graph outputs.\n\n\n\n2. Analysing the global and local spatial associations of medical resources across Wuhan city using POI data (Chen, Q et al., 2023)\nThis article highlights the issue of imbalance in the supply and demand of medical resources in provincial capitals of China and emphasises the need to understand the spatial patterns of medical resources to ensure fair and optimal allocation of limited resources. This article utilises the Localised Colocation Quotient (LCLQ) analysis, which is a technique that measures directional spatial associations and heterogeneity between categorical point data. By employing this method and utilising point of interest (POI) data, the study presents a unique analysis of the spatial patterns and directional spatial associations between six medical resources in Wuhan city.\n\nHow does it link to our project?\nFor our project, we need to first define the two types of features of interest which will be HDB locations and the presence of certain amenities. Some examples are schools, carparks, shopping centres and MRT stations. By comparing the observed and expected frequencies of co-occurrence using the LCLQ, we can determine whether HDB locations and surrounding amenities are spatially associated in a non-random way. This can provide insights into the degree to which the availability of amenities in an area affects HDB locations, and vice versa.\n\n\n\n3. Measuring Spatial Patterns of Health Care Facilities and Their Relationships with Hypertension Inpatients in a Network-Constrained Urban System (Wang and Ke, 2019)\nThis study applied point pattern analysis (PPA) to investigate the spatial distribution of health care facilities in Shenzhen, China. Traditional PPA methods assume that spatial events are randomly located on a plane, but this is not appropriate for network-constrained events, such as those that occur on urban road networks. Therefore, the study used network-based analysis methods, such as network Kernel density estimation and network K-function.\n\nHow does it link to our project?\nWe plan to utilise the netKDE techniques employed in the article to estimate the density of events, such as the density of HDB locations, within a network-constrained environment. The purpose of using netKDE for HDB locations is to identify areas of high or low concentration of HDB units based on their proximity to the road network. With this analysis, HDB buyers can understand the density of HDB units in different regions of Singapore by identifying regions with a high concentration of HDB units, which may be indicative of a high demand for housing in those areas."
  },
  {
    "objectID": "project_proposal.html#our-approach",
    "href": "project_proposal.html#our-approach",
    "title": "G1T7 Project/Research Paper",
    "section": "Our Approach",
    "text": "Our Approach\n\nData Preparation\n\nAssemble data from various sources\nData handling and wrangling\n\nExploratory Data Analysis\n\nAnalyse the spatial arrangement of data points\nIdentify any outliers or anomalies in the data sets\nChoropleth Mapping\n\nSpatial Point Patterns Analysis (Filter by: Sub-district/Region)\n\nFirst point Analysis\n\nKernel Density Estimation:\n\nSecond point Analysis\n\nF-Function: Identify whether the distribution of amenities around HDBs is random or clustered, and the ratio of observed to expected nearest neighbor distances.\nRipley’s K-function and L function: To measure the degree of clustering or dispersion of HDB locations and surrounding amenities, and help identify significant spatial patterns and trends.\nColocation Quotients CLQs: To measure the extent of spatial association and heterogeneity between HDB locations and surrounding amenities, helping to identify areas of high or low co-location.\nNetwork Constrained Spatial Point Patterns Analysis: To analyse the spatial distribution of HDB flats over a street network"
  },
  {
    "objectID": "project_proposal.html#packages",
    "href": "project_proposal.html#packages",
    "title": "G1T7 Project/Research Paper",
    "section": "Packages",
    "text": "Packages\nBelow is the list of packages we will be using for our project:\n\nsf: import and handle geospatial data\ntidyverse: data wrangling (tidyr, dplyr, ggplot2, tibble)\ntmap: plot choropleth maps\nmaptools: a set of tools for manipulating geographic data\nraster: convert grid output to raster layer for visualisation (Kernel Density Estimation)\nspatstat: conversion from spatial object to ppp format (Spatial Point Pattern plot)\nfunModeling: plotting EDA"
  },
  {
    "objectID": "project_proposal.html#application-system-architecture",
    "href": "project_proposal.html#application-system-architecture",
    "title": "G1T7 Project Research Paper",
    "section": "Application System Architecture",
    "text": "Application System Architecture\n\n\n\nFigure 9: Application Architecture Design"
  },
  {
    "objectID": "project_proposal.html#timeline",
    "href": "project_proposal.html#timeline",
    "title": "G1T7 Project Research Paper",
    "section": "Timeline",
    "text": "Timeline"
  },
  {
    "objectID": "project_proposal.html#early-storyboard-drafts",
    "href": "project_proposal.html#early-storyboard-drafts",
    "title": "G1T7 Project/Research Paper",
    "section": "Early Storyboard Drafts",
    "text": "Early Storyboard Drafts\nWe used Figma to create a draft designs of our storyboard on how we imagine our Shiny app to look like. Please click here to view it in Figma."
  },
  {
    "objectID": "project_proposal.html#references",
    "href": "project_proposal.html#references",
    "title": "G1T7 Project Research Paper",
    "section": "References",
    "text": "References\nSpatial First Point Analysis (link)\nSpatial Second Point Analysis (link)\nCLQ Interpretation (link)\nNetwork Constrained Spatial Point Patterns Analysis (link)\nSenior’s Interactive L-function code (link)\nShiny Documentation (link)\nLiterature review No.1 (link)\nLiterature review No.2 (link)\nLiterature review No.3 (link)"
  },
  {
    "objectID": "code_draft/data/geospatial/MPSZ-2019.html",
    "href": "code_draft/data/geospatial/MPSZ-2019.html",
    "title": "",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "code_draft/data_pre_processing.html",
    "href": "code_draft/data_pre_processing.html",
    "title": "Data Pre-processing",
    "section": "",
    "text": "pacman::p_load(readxl, sf, tidyverse, tmap, sfdep, rvest, httr, jsonlite, onemapsgapi, ggpubr, olsrr,ggplot2, plotly)"
  },
  {
    "objectID": "code_draft/data_pre_processing.html#resale-flat-prices-aspatial",
    "href": "code_draft/data_pre_processing.html#resale-flat-prices-aspatial",
    "title": "Data Pre-processing",
    "section": "Resale Flat Prices (Aspatial)",
    "text": "Resale Flat Prices (Aspatial)\nExtract locations over study area 12 months, January 2022 to December 2022\n\nresale <- read_csv(\"data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv\")\n\n\n#extract HDB study are of January 2022 to December 2022\nresale<- resale %>% \n  filter(flat_type == \"5 ROOM\") %>%\n  filter(month >= \"2022-01\" & month <= \"2022-12\")\n\n\nresale$street_name <- gsub(\"ST\\\\.\", \"SAINT\", resale$street_name)\n\n#filter only unique HDB blocks\nresale %>% distinct(street_name, block)\n\n\ngeocode <- function(block, streetname) {\n  base_url <- \"https://developers.onemap.sg/commonapi/search\"\n  address <- paste(block, streetname, sep = \" \")\n  query <- list(\"searchVal\" = address, \n                \"returnGeom\" = \"Y\",\n                \"getAddrDetails\" = \"N\",\n                \"pageNum\" = \"1\")\n  \n  res <- GET(base_url, query = query)\n  restext<-content(res, as=\"text\")\n  \n  output <- fromJSON(restext)  %>% \n    as.data.frame %>%\n    select(results.LATITUDE, results.LONGITUDE)\n\n  return(output)\n}\n\n\nresale$LATITUDE <- 0\nresale$LONGITUDE <- 0\n\nfor (i in 1:nrow(resale)){\n  temp_output <- geocode(resale[i, 4], resale[i, 5])\n  \n  resale$LATITUDE[i] <- temp_output$results.LATITUDE\n  resale$LONGITUDE[i] <- temp_output$results.LONGITUDE\n}\n\n\n# resale flat with lat and long\nwrite_rds(resale, \"data/rds/resale_locations(1year).rds\")\n\n\nresale_locations<-readRDS(\"data/rds/resale_locations(1year).rds\")\n\n#Transform\nresale_sf <- st_as_sf(resale_locations, \n                      coords = c(\"LONGITUDE\", \n                                 \"LATITUDE\"), \n                      crs=4326) %>%\n  st_transform(crs = 3414)\n\n#Missing values\nsum(is.na(resale_sf))\n\n#correct CRS\nst_crs(resale_sf)\n\n\n#invalid geometries\nlength(which(st_is_valid(resale_sf) == FALSE))\n\n\nwrite_rds(resale_sf, \"data/rds/resale_sf.rds\")\n\n\n# based on visualisations, realised an area is placed wrongly (api gave wrong coordinates, thus, i removed it)\n\n#resale_sf<- subset(resale_sf, full_address != \"BLK 27 MARINE CRES\")"
  },
  {
    "objectID": "code_draft/data_pre_processing.html#master-plan-2019-boundary",
    "href": "code_draft/data_pre_processing.html#master-plan-2019-boundary",
    "title": "Data Pre-processing",
    "section": "Master Plan 2019 Boundary",
    "text": "Master Plan 2019 Boundary\n\nmpsz <- st_read(dsn=\"data/geospatial\", layer=\"MPSZ-2019\") %>%\n  st_transform(crs=3414)"
  },
  {
    "objectID": "code_draft/data_pre_processing.html#locational-factors-extracted-via-onemapapi-token",
    "href": "code_draft/data_pre_processing.html#locational-factors-extracted-via-onemapapi-token",
    "title": "Data Pre-processing",
    "section": "Locational Factors Extracted via onemapAPI token",
    "text": "Locational Factors Extracted via onemapAPI token\n\ntoken <- get_token(\"email\", \"password\")\n\n\navail_themes <-search_themes(token)\nwrite_rds(avail_themes, \"data/rds/available_themes.rds\")\n\n\n#read the file for available themes\navail_themes<-readRDS(\"data/rds/available_themes.rds\")\n\n#sort by alphabetical order\navail_themes<-avail_themes[order(avail_themes$THEMENAME),]\navail_themes\n\n\n#childcare\n#retrieve the data such as the geometry and name  accordingly to the theme\nchildcare_tibble <- get_theme(token, \"childcare\")\n\n# to convert a data frame of coordinates to an sf object and transform the crs information and create a shapefile for it\nchildcare_sf <- st_as_sf(childcare_tibble, coords=c(\"Lng\", \"Lat\"),\n                        crs=4326) %>% \n  st_transform(crs = 3414)\n\nwrite_rds(childcare_sf, \"data/rds/childcare_sf.rds\")\n\n\n#eldercare\neldercare_tibble <- get_theme(token, \"eldercare\")\n\neldercare_sf <- st_as_sf(eldercare_tibble, coords=c(\"Lng\", \"Lat\"),\n                         crs=4326) %>% \n  st_transform(crs = 3414)\n\nwrite_rds(eldercare_sf, \"data/rds/eldercare_sf.rds\")\n\n\n#kindergatens\nkindergartens_tibble <- get_theme(token, \"kindergartens\")\n\nkindergartens_sf <- st_as_sf(kindergartens_tibble, coords=c(\"Lng\", \"Lat\"), \n                        crs=4326) %>% \n  st_transform(crs = 3414)\nwrite_rds(kindergartens_sf, \"data/rds/kindergartens_sf.rds\")\n\n#hawker Centres\nhawkercentre_new_tibble <- get_theme(token, \"hawkercentre_new\")\n\nhawkercentre_new_sf <- st_as_sf(hawkercentre_new_tibble, coords=c(\"Lng\", \"Lat\"), \n                        crs=4326) %>% \n  st_transform(crs = 3414)\n\nwrite_rds(hawkercentre_new_sf, \"data/rds/hawkercentre_new_sf.rds\")\n\n\n#Healthier Hawker Centres, healthier_hawker_centres\nhawkercentre_healthy_tibble <- get_theme(token, \"healthier_hawker_centres\")\n\nhawkercentre_healthy_sf <- st_as_sf(hawkercentre_healthy_tibble, coords=c(\"Lng\", \"Lat\"), \n                        crs=4326) %>% \n  st_transform(crs = 3414)\n\nwrite_rds(hawkercentre_healthy_sf, \"data/rds/hawkercentre_healthy_sf.rds\")\n\n#parks\nnationalparks_tibble <- get_theme(token, \"nationalparks\")\n\nnationalparks_sf <- st_as_sf(nationalparks_tibble, \n                             coords=c(\"Lng\", \"Lat\"), crs=4326) %>%\n  st_transform(crs = 3414)\n\nwrite_rds(nationalparks_sf, \"data/rds/nationalparks_sf.rds\")\n\n#Gyms@SG, exercisefacilities\ngyms_tibble <- get_theme(token, \"exercisefacilities\")\n\ngyms_sf <- st_as_sf(gyms_tibble, \n                             coords=c(\"Lng\", \"Lat\"), crs=4326) %>%\n  st_transform(crs = 3414)\n\nwrite_rds(gyms_sf, \"data/rds/gyms_sf.rds\")\n\n#Retail pharmacy locations , registered_pharmacy\npharmacy_tibble <- get_theme(token, \"registered_pharmacy\")\n\npharmacy_sf <- st_as_sf(pharmacy_tibble, \n                             coords=c(\"Lng\", \"Lat\"), crs=4326) %>%\n  st_transform(crs = 3414)\n\nwrite_rds(pharmacy_sf, \"data/rds/pharmacy_sf.rds\")\n\n#spf establishments, spf_establishments\nspf_tibble <- get_theme(token, \"spf_establishments\")\n\nspf_sf <- st_as_sf(spf_tibble, \n                             coords=c(\"Lng\", \"Lat\"), crs=4326) %>%\n  st_transform(crs = 3414)\n\nwrite_rds(spf_sf, \"data/rds/spf_sf.rds\")\n\n\n#HDB Car Park Information,  hdb_car_park_information\nHDB_carpark_tibble <- get_theme(token, \"hdb_car_park_information\")\n\nHDB_carpark_sf <- st_as_sf(HDB_carpark_tibble, \n                             coords=c(\"Lng\", \"Lat\"), crs=4326) %>%\n  st_transform(crs = 3414)\n\nwrite_rds(HDB_carpark_sf, \"data/rds/HDB_carpark_sf.rds\")"
  },
  {
    "objectID": "code_draft/data_pre_processing.html#supermarket",
    "href": "code_draft/data_pre_processing.html#supermarket",
    "title": "Data Pre-processing",
    "section": "Supermarket",
    "text": "Supermarket\n\nsupermarket_sf <- st_read(\"data/geospatial/supermarkets-geojson.geojson\") \nsupermarket_sf <- supermarket_sf %>%\n  st_transform(crs = 3414)\n\n\nwrite_rds(supermarket_sf, \"data/rds/supermarket_sf.rds\")"
  },
  {
    "objectID": "code_draft/data_pre_processing.html#bus-stop",
    "href": "code_draft/data_pre_processing.html#bus-stop",
    "title": "Data Pre-processing",
    "section": "Bus Stop",
    "text": "Bus Stop\n\nbus_stop<- st_read(dsn = \"data/geospatial\", layer = \"BusStop\")\nbus_stop_sf <- bus_stop %>%\n  st_transform(crs = 3414)\n\n\nwrite_rds(bus_stop_sf, \"data/rds/bus_stop_sf.rds\")"
  },
  {
    "objectID": "code_draft/data_pre_processing.html#mrtlrt",
    "href": "code_draft/data_pre_processing.html#mrtlrt",
    "title": "Data Pre-processing",
    "section": "MRT/LRT",
    "text": "MRT/LRT\n\nmrt = st_read(dsn = \"data/geospatial/\", layer = \"Train_Station_Exit_Layer\")\nmrt_sf <- mrt %>%\n  st_transform(crs = 3414)\n\n\nwrite_rds(mrt_sf, \"data/rds/mrt_sf.rds\")"
  },
  {
    "objectID": "code_draft/data_pre_processing.html#primary-school",
    "href": "code_draft/data_pre_processing.html#primary-school",
    "title": "Data Pre-processing",
    "section": "Primary School",
    "text": "Primary School\n\nprimary_school <- read_csv(\"data/aspatial/general-information-of-schools.csv\")\n\nprimary_school <- primary_school %>%\n  filter(mainlevel_code == \"PRIMARY\") %>%\n  select(school_name, address, postal_code)\n\nprimary_school<-primary_school %>%  \n  filter(school_name!='JUYING PRIMARY SCHOOL')\n\n\nprimary_school$LATITUDE <- 0\nprimary_school$LONGITUDE <- 0\n\nfor (i in 1:nrow(primary_school)){\n  temp_output <- geocode(primary_school[i, 1],\"\")\n\n  primary_school$LATITUDE[i] <- temp_output$results.LATITUDE\n  primary_school$LONGITUDE[i] <- temp_output$results.LONGITUDE\n}\n\n\nwrite_rds(primary_school, \"data/rds/primary_school_locations.rds\")\n\n\nprimary_school_sf <- st_as_sf(primary_school,\n                    coords = c(\"LONGITUDE\", \n                               \"LATITUDE\"),\n                    crs=4326) %>%\n  st_transform(crs = 3414)\n\n\nwrite_rds(primary_school_sf, \"data/rds/primary_school_sf.rds\")"
  },
  {
    "objectID": "code_draft/data_pre_processing.html#top-10-primary-schools",
    "href": "code_draft/data_pre_processing.html#top-10-primary-schools",
    "title": "Data Pre-processing",
    "section": "Top 10 Primary Schools",
    "text": "Top 10 Primary Schools\n\n#reference to:https://schoolbell.sg/primary-school-ranking/\npopular_primary_schools <-c(\"Pei Hwa Presbyterian Primary School\",\n                            \"Gongshang Primary School\",\n                            \"Riverside Primary School\",\n                            \"Red Swastika School\",\n                            \"Punggol Green Primary School\",\n                            \"Princess Elizabeth Primary School\",\n                            \"Westwood Primary School\",\n                            \"St. Hilda’s Primary School\",\n                            \"Catholic High School (Primary Section)\",\n                            \"Ai Tong School\")\n\n#make school names all uppercase\npopular_primary_schools <- lapply(popular_primary_schools, toupper) \n\n# to check both primary school datasets matches\npopular_primary_schools_sf <- primary_school_sf %>%\n  filter(school_name %in% popular_primary_schools)\n\npopular_primary_schools_sf <- popular_primary_schools_sf %>%\n  rbind(primary_school_sf %>% filter(school_name == \"CANOSSA CATHOLIC PRIMARY SCHOOL\"))\n\npopular_primary_schools_sf <- popular_primary_schools_sf %>%\n  rbind(primary_school_sf %>% filter(school_name == \"ST. HILDA'S PRIMARY SCHOOL\"))\n\nnrow(popular_primary_schools_sf)\n\n\nwrite_rds(popular_primary_schools_sf, \"data/rds/top10_primary_school_sf.rds\")"
  },
  {
    "objectID": "code_draft/data_pre_processing.html#shopping-mall",
    "href": "code_draft/data_pre_processing.html#shopping-mall",
    "title": "Data Pre-processing",
    "section": "Shopping Mall",
    "text": "Shopping Mall\n\nshopping_mall <- read.csv(\"data/geospatial/mall_coordinates_updated.csv\")\n\nshopping_mall <- shopping_mall %>%\n  select(name, latitude, longitude)\n\nshopping_mall_sf <- st_as_sf(shopping_mall,\n                        coords = c(\"longitude\",\n                                   \"latitude\"),\n                        crs = 4326) %>%\n  st_transform(crs = 3414)\n\n\nwrite_rds(shopping_mall_sf, \"data/rds/shopping_mall_sf.rds\")\n\nPreprocess\n\n#remove irrelevant columns\n\n#childcare_sf\nchildcare_sf <- childcare_sf %>% select(\"NAME\")\n\n#eldercare_sf\neldercare_sf <- eldercare_sf %>% select(\"NAME\")\n\n#kindergartens_sf\nkindergartens_sf <- kindergartens_sf %>% select(\"NAME\")\n\n#hawkercentre_new_sf\nhawkercentre_new_sf <- hawkercentre_new_sf %>% select(\"NAME\")\n\n#hawkercentre_healthy_sf \nhawkercentre_healthy_sf  <- hawkercentre_healthy_sf  %>% select(\"NAME\")\n\n#nationalparks_sf\nnationalparks_sf <- nationalparks_sf %>% select(\"NAME\")\n\n#gyms_sf\ngyms_sf <- gyms_sf %>% select(\"NAME\")\n#pharmacy_sf\npharmacy_sf <- pharmacy_sf %>% select(\"NAME\")\n#spf_sf\nspf_sf <- spf_sf %>% select(\"NAME\")\n#HDB_carpark_sf\nHDB_carpark_sf <- HDB_carpark_sf %>% select(\"NAME\",\"CAR_PARK_TYPE\",\"TYPE_OF_PARKING_SYSTEM\", \"NIGHT_PARKING\", \"FREE_PARKING\")\n\n#supermarket\nsupermarket_sf$LIC_NAME <- str_extract(supermarket_sf$Description, \"(?<=LIC_NAME<\\\\/th> <td>)[^<]+\")\nsupermarket_sf <- supermarket_sf %>% select(\"LIC_NAME\")\n\n#bus stop\nbus_stop_sf$stop_name <- paste(bus_stop_sf$BUS_STOP_N, bus_stop_sf$BUS_ROOF_N, bus_stop_sf$LOC_DESC)\nbus_stop_sf <- bus_stop_sf %>% select(\"stop_name\")\n\n#mrt\n#combine stn name and exit to make each row unique\nmrt_sf$stn <- paste(mrt_sf$stn_name, mrt_sf$exit_code)\nmrt_sf <- mrt_sf %>% select(\"stn\")\n\n\n#missing values\nsum(is.na(mpsz))\nsum(is.na(childcare_sf))\nsum(is.na(eldercare_sf))\nsum(is.na(kindergartens_sf))\nsum(is.na(hawkercentre_new_sf))\nsum(is.na(hawkercentre_healthy_sf))\nsum(is.na(nationalparks_sf))\nsum(is.na(gyms_sf))\nsum(is.na(pharmacy_sf))\nsum(is.na(spf_sf))\nsum(is.na(HDB_carpark_sf))\nsum(is.na(supermarket_sf))\nsum(is.na(bus_stop_sf))\nsum(is.na(mrt_sf))\nsum(is.na(primary_school_sf))\nsum(is.na(popular_primary_schools_sf))\nsum(is.na(shopping_mall_sf))\n\n\nlength(which(st_is_valid(mpsz) == FALSE))\nlength(which(st_is_valid(childcare_sf) == FALSE))\nlength(which(st_is_valid(eldercare_sf) == FALSE))\nlength(which(st_is_valid(kindergartens_sf) == FALSE))\nlength(which(st_is_valid(hawkercentre_new_sf) == FALSE))\nlength(which(st_is_valid(hawkercentre_healthy_sf) == FALSE))\nlength(which(st_is_valid(nationalparks_sf) == FALSE))\nlength(which(st_is_valid(gyms_sf) == FALSE))\nlength(which(st_is_valid(pharmacy_sf) == FALSE))\nlength(which(st_is_valid(HDB_carpark_sf) == FALSE))\nlength(which(st_is_valid(supermarket_sf) == FALSE))\nlength(which(st_is_valid(bus_stop_sf) == FALSE))\nlength(which(st_is_valid(mrt_sf) == FALSE))\nlength(which(st_is_valid(primary_school_sf) == FALSE))\nlength(which(st_is_valid(popular_primary_schools_sf) == FALSE))\nlength(which(st_is_valid(shopping_mall_sf) == FALSE))\n\n\nmpsz <- st_make_valid(mpsz)\nlength(which(st_is_valid(mpsz) == FALSE))"
  },
  {
    "objectID": "code_draft/data_pre_processing.html#rewrite-sf.rds-files-with-processed-ver",
    "href": "code_draft/data_pre_processing.html#rewrite-sf.rds-files-with-processed-ver",
    "title": "Data Pre-processing",
    "section": "Rewrite sf.rds files with processed ver",
    "text": "Rewrite sf.rds files with processed ver\n\nwrite_rds(mpsz, \"data/rds/mpsz_sf.rds\")\nwrite_rds(childcare_sf, \"data/rds/childcare_sf.rds\")\nwrite_rds(eldercare_sf, \"data/rds/eldercare_sf.rds\")\nwrite_rds(kindergartens_sf, \"data/rds/kindergartens_sf.rds\")\nwrite_rds(hawkercentre_new_sf, \"data/rds/hawkercentre_new_sf.rds\")\nwrite_rds(hawkercentre_healthy_sf, \"data/rds/hawkercentre_healthy_sf.rds\")\nwrite_rds(nationalparks_sf, \"data/rds/nationalparks_sf.rds\")\nwrite_rds(gyms_sf, \"data/rds/gyms_sf.rds\")\nwrite_rds(pharmacy_sf, \"data/rds/pharmacy_sf.rds\")\nwrite_rds(spf_sf, \"data/rds/spf_sf.rds\")\nwrite_rds(HDB_carpark_sf, \"data/rds/HDB_carpark_sf.rds\")\nwrite_rds(supermarket_sf, \"data/rds/supermarket_sf.rds\")\nwrite_rds(bus_stop_sf, \"data/rds/bus_stop_sf.rds\")\nwrite_rds(mrt_sf, \"data/rds/mrt_sf.rds\")\nwrite_rds(primary_school_sf, \"data/rds/primary_school_sf.rds\")\nwrite_rds(popular_primary_schools_sf, \"data/rds/top10_primary_school_sf.rds\")\nwrite_rds(shopping_mall_sf, \"data/rds/shopping_mall_sf.rds\")"
  },
  {
    "objectID": "code_draft/mapping.html",
    "href": "code_draft/mapping.html",
    "title": "Mapping and LCLQ",
    "section": "",
    "text": "pacman::p_load(readxl, sf, tidyverse, tmap, sfdep,  ggpubr, plotly, sfdep, data.table)"
  },
  {
    "objectID": "code_draft/mapping.html#import-data",
    "href": "code_draft/mapping.html#import-data",
    "title": "Mapping and LCLQ",
    "section": "Import Data",
    "text": "Import Data\n\n#import data\nresale_sf<-readRDS(\"data/rds/resale_sf.rds\")\n\nmpsz_sf<-readRDS(\"data/rds/mpsz_sf.rds\")\n\nchildcare_sf <- readRDS(\"data/rds/childcare_sf.rds\")\neldercare_sf<- readRDS(\"data/rds/eldercare_sf.rds\")\nkindergartens_sf <- readRDS(\"data/rds/kindergartens_sf.rds\")\nhawkercentre_new_sf <- readRDS(\"data/rds/hawkercentre_new_sf.rds\")\nhawkercentre_healthy_sf<-readRDS(\"data/rds/hawkercentre_healthy_sf.rds\")\nnationalparks_sf<-readRDS(\"data/rds/nationalparks_sf.rds\")\ngyms_sf <-readRDS(\"data/rds/gyms_sf.rds\")\npharmacy_sf<-readRDS(\"data/rds/pharmacy_sf.rds\")\nspf_sf<-readRDS(\"data/rds/spf_sf.rds\")\nHDB_carpark_sf<-readRDS(\"data/rds/HDB_carpark_sf.rds\")\n\nsupermarket_sf<-readRDS(\"data/rds/supermarket_sf.rds\")\nbus_stop_sf<-readRDS(\"data/rds/bus_stop_sf.rds\")\nmrt_sf<-readRDS(\"data/rds/mrt_sf.rds\")\nprimary_school_sf<-readRDS(\"data/rds/primary_school_sf.rds\")\ntop10_primary_school_sf<-readRDS(\"data/rds/top10_primary_school_sf.rds\")\nshopping_mall_sf<-readRDS(\"data/rds/shopping_mall_sf.rds\")\n\n# combine street name and blk for HDB full address\nresale_sf$full_address <- paste(\"BLK\", resale_sf$block, resale_sf$street_name)"
  },
  {
    "objectID": "code_draft/mapping.html#mapping",
    "href": "code_draft/mapping.html#mapping",
    "title": "Mapping and LCLQ",
    "section": "Mapping",
    "text": "Mapping\nHDB locations and relevant information\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\n  tm_shape(resale_sf) +\n  tm_dots(col = \"resale_price\", \n          id = \"full_address\", # bold in popup\n          popup.vars = c(\"Resale Price:\" = \"resale_price\",\n                         \"Flat Type:\" = \"flat_type\", \n                         \"Flat Model:\" = \"flat_model\",\n                         \"Floor Area (sqm):\" = \"floor_area_sqm\",\n                         \"Remaining Lease:\" = \"remaining_lease\"\n                         ),\n          title = \"Resale Prices\")\n\nHDB and Supermarket\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\n  tm_shape(resale_sf) +\n  tm_dots(col = \"resale_price\", \n          id = \"full_address\", # bold in popup\n          popup.vars = c(\"Resale Price:\" = \"resale_price\",\n                         \"Flat Type:\" = \"flat_type\", \n                         \"Flat Model:\" = \"flat_model\",\n                         \"Floor Area (sqm):\" = \"floor_area_sqm\",\n                         \"Remaining Lease:\" = \"remaining_lease\"\n                         ),\n          title = \"Resale Prices\") +\n  tm_shape(supermarket_sf) +\n  tm_dots(alpha=0.5,\n        col=\"#FF0000\",\n        size=0.05) +\n  tm_view(set.zoom.limits = c(10, 16))\n\nHDB and Kindergartens\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\ntm_shape(resale_sf_address) +\n  tm_dots(alpha=0.5, #affects transparency of points\n          col=\"#FCE883\",\n          size=0.05)+\ntm_shape(kindergartens_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#FFD1DC\",\n          size=0.05)\n\nEarly Childhood/Primary Schools\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.5, #affects transparency of points\n          col=\"#FCE883\",\n          size=0.05)+\ntm_shape(kindergartens_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#FFD1DC\",\n          size=0.05) +\ntm_shape(primary_school_sf) +\n  tm_dots(alpha=0.5,\n        col=\"#FF0000\",\n        size=0.05) +\n  tm_view(set.zoom.limits = c(10, 16))\n\nTransport (Bus)\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\ntm_shape(bus_stop_sf) +\n  tm_dots(alpha=0.5, #affects transparency of points\n          col=\"#007FFF\",\n          size=0.05)\n  tm_view(set.zoom.limits = c(10, 14))\n\nTransport(Train)\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) + \ntm_shape(mrt_sf)+ \n  tm_dots(col = \"red\",\n             size = 0.01,\n             border.col = \"black\",\n             border.lwd = 0.5,\n           legend.show = FALSE) +\n  tm_view(set.zoom.limits = c(10, 16))\n\nHDB and another amenity function\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\n  tm_shape(resale_sf) +\n  tm_dots(col = \"resale_price\", \n          id = \"full_address\", # bold in popup\n          popup.vars = c(\"Resale Price:\" = \"resale_price\",\n                         \"Flat Type:\" = \"flat_type\", \n                         \"Flat Model:\" = \"flat_model\",\n                         \"Floor Area (sqm):\" = \"floor_area_sqm\",\n                         \"Remaining Lease:\" = \"remaining_lease\"\n                         ),\n          title = \"Resale Prices\") +\n  tm_shape(supermarket_sf) +\n  tm_dots(alpha=0.5,\n        col=\"#FF0000\",\n        size=0.05) +\n  tm_view(set.zoom.limits = c(10, 16))"
  },
  {
    "objectID": "code_draft/mapping.html#lclq",
    "href": "code_draft/mapping.html#lclq",
    "title": "Mapping and LCLQ",
    "section": "LCLQ",
    "text": "LCLQ\n\n#HDB locations\nhdb_a <- resale_sf %>%\n  select(geometry) %>% \n  mutate(geometry= geometry,\n         type = \"HDB\") \n\n\n#formula to get the LCLQ data\n\n#hdb_a, amenity df, amenity name\ncql_func <- function(df1,df2,nameB){\n  df2_b <- df2 %>%\n  select(geometry) %>% \n  mutate(geometry= geometry,\n         type = nameB) \n  combined <- rbindlist(list(df1, df2_b),use.names=FALSE)\n  combined_sf <- st_as_sf(combined) %>%\n  st_transform(crs = 3414)\n  \n  nb <- include_self(\n  st_knn(st_geometry(combined_sf), 6))\n  \n  wt <- st_kernel_weights(nb, \n                        combined_sf, \n                        \"gaussian\", \n                        adaptive = TRUE)\n  \n  hdb_locations <- combined_sf %>%\n  filter(type == \"HDB\")\n  A <- hdb_locations$type\n  \n  df2_locations <- combined_sf %>%\n  filter(type == nameB)\n  \n  B <- df2_locations$type\n  \n  LCLQ <- local_colocation(A, B, wt, nb, 99) # 100 simulations\n  LCLQ_stores <- cbind(combined_sf, LCLQ)\n  \n  write_rds(LCLQ_stores, paste0(\"data/rds/clq/hdb_\",nameB,\".rds\"))\n  }\n\n\n# dont run again\ncql_func(hdb_a,childcare_sf,\"Childcare\")\ncql_func(hdb_a,eldercare_sf,\"Eldercare\")\ncql_func(hdb_a,kindergartens_sf,\"Kindegarten\")\ncql_func(hdb_a,nationalparks_sf,\"NationalParks\")\ncql_func(hdb_a,gyms_sf,\"Gym\")\ncql_func(hdb_a,pharmacy_sf,\"Pharmacy\")\ncql_func(hdb_a,supermarket_sf,\"Supermarket\")\ncql_func(hdb_a,bus_stop_sf,\"Bus\")\ncql_func(hdb_a,HDB_carpark_sf,\"Carparks\")\ncql_func(hdb_a,mrt_sf,\"Mrt\")\ncql_func(hdb_a,primary_school_sf,\"PrimarySchool\")\ncql_func(hdb_a,hawkercentre_new_sf,\"Hawker\")\ncql_func(hdb_a,shopping_mall_sf,\"ShoppingMall\")"
  },
  {
    "objectID": "code_draft/mapping.html#import-lclq-data",
    "href": "code_draft/mapping.html#import-lclq-data",
    "title": "Mapping and LCLQ",
    "section": "Import LCLQ Data",
    "text": "Import LCLQ Data\n\n# import the clq files\nchildcare_clq<- readRDS(\"data/rds/clq/hdb_Childcare.rds\")\neldercare_clq<- readRDS(\"data/rds/clq/hdb_Eldercare.rds\")\nkindergarten_clq<- readRDS(\"data/rds/clq/hdb_Kindegarten.rds\")\nnationalParks_clq<- readRDS(\"data/rds/clq/hdb_NationalParks.rds\")\ngym_clq<- readRDS(\"data/rds/clq/hdb_Gym.rds\")\npharmacy_clq<- readRDS(\"data/rds/clq/hdb_Pharmacy.rds\")\nsupermarket_clq<- readRDS(\"data/rds/clq/hdb_Supermarket.rds\")\nbus_clq<- readRDS(\"data/rds/clq/hdb_Bus.rds\")\ncarpark_clq<- readRDS(\"data/rds/clq/hdb_Carparks.rds\")\nmrt_clq<- readRDS(\"data/rds/clq/hdb_Mrt.rds\")\nprimarySchool_clq<- readRDS(\"data/rds/clq/hdb_PrimarySchool.rds\")\nhawker_clq<- readRDS(\"data/rds/clq/hdb_Hawker.rds\")\nshoppingMall_clq<- readRDS(\"data/rds/clq/hdb_ShoppingMall.rds\")\n\n#\nmpsz_sf<-readRDS(\"data/rds/mpsz_sf.rds\")\n\n\n#check pvalues\n\n unique(st_drop_geometry(childcare_clq)[, 3])\n unique(st_drop_geometry(eldercare_clq)[, 3])\n unique(st_drop_geometry(kindergarten_clq)[, 3])\n unique(st_drop_geometry(nationalParks_clq)[, 3])\n unique(st_drop_geometry(gym_clq)[, 3])\n unique(st_drop_geometry(pharmacy_clq)[, 3])\n unique(st_drop_geometry(supermarket_clq)[, 3])\n unique(st_drop_geometry(bus_clq)[, 3]) # has p-value above 0.05\n unique(st_drop_geometry(carpark_clq)[, 3])\n unique(st_drop_geometry(mrt_clq)[, 3])\n unique(st_drop_geometry(primarySchool_clq)[, 3])\n unique(st_drop_geometry(hawker_clq)[, 3])\n unique(st_drop_geometry(shoppingMall_clq)[, 3])\n\n\n#intepretation reference: https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/learnmorecolocationanalysis.htm\n\n# filtered_data <- subset(eldercare_clq, p_sim_Eldercare < 0.05)\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons() +\ntm_shape(bus_clq)+ \n  tm_dots(col = \"Bus\",\n             size = 0.01,\n             border.col = \"black\",\n             border.lwd = 0.5) \n#+ tm_view(set.zoom.limits = c(12, 16))"
  },
  {
    "objectID": "the-right-space/data/geospatial/MPSZ-2019.html",
    "href": "the-right-space/data/geospatial/MPSZ-2019.html",
    "title": "",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415 AY22-23 Project",
    "section": "",
    "text": "About Us\nHello! We are G1T7, Nguyen Mai Phuong, Kwan Kai Xuan Belle and Rhonda Ho Kah Yee.\n\n\nOur Purpose\nThis project is created for an IS415 Geospatial Analytics & Application, a module in Singapore Management University (SMU) with the guidance of Professor Kam Tin Seong.\n\n\nOur Project Theme and Goal\nThe theme for this project is Spatial Point Analysis, and our goal is to provide a user-friendly analytical app that simplifies the process of understanding the amenities available in the vicinity of prospective HDB buyers. By utilizing advanced spatial point pattern analysis techniques, the app visualizes the distribution and clustering of relevant amenities, providing a tailored view that empowers HDB buyers to make informed decisions and feel confident in their chosen residence. To view more of our project deliverable, click here."
  },
  {
    "objectID": "code_draft/network_constraint.html",
    "href": "code_draft/network_constraint.html",
    "title": "Network Constrained SP Pattern Analysis",
    "section": "",
    "text": "pacman::p_load(sp, sf, rgdal, spNetwork, tmap, tidyverse)\n\n\n\n\n\narea_names <- c(\"MARINA EAST\",\"RIVER VALLEY\", \"SINGAPORE RIVER\",\"WESTERN ISLANDS\", \"MUSEUM\",\"MARINE PARADE\", \"SOUTHERN ISLANDS\",\"BUKIT MERAH\", \"DOWNTOWN CORE\",\"STRAITS VIEW\", \"QUEENSTOWN\",\"OUTRAM\", \"MARINA SOUTH\",\"ROCHOR\", \"KALLANG\",\"TANGLIN\", \"NEWTON\",\"CLEMENTI\", \"BEDOK\",\"PIONEER\", \"JURONG EAST\",\"ORCHARD\", \"GEYLANG\",\"BOON LAY\", \"BUKIT TIMAH\",\"NOVENA\", \"TOA PAYOH\",\"TUAS\", \"JURONG WEST\",\"SERANGOON\", \"BISHAN\",\"TAMPINES\", \"BUKIT BATOK\",\"HOUGANG\", \"CHANGI BAY\",\"PAYA LEBAR\", \"ANG MO KIO\",\"PASIR RIS\", \"BUKIT PANJANG\",\"TENGAH\", \"SELETAR\",\"SUNGEI KADUT\", \"YISHUN\",\"MANDAI\", \"PUNGGOL\",\"CHOA CHU KANG\", \"SENGKANG\",\"CHANGI\", \"CENTRAL WATER CATCHMENT\",\"SEMBAWANG\", \"WESTERN WATER CATCHMENT\",\"WOODLANDS\", \"NORTH-EASTERN ISLANDS\",\"SIMPANG\", \"LIM CHU KANG\")\narea_names <- gsub(\" \", \"_\", area_names)\n\n\narea_names <- c(\"MARINE PARADE\", \"BUKIT MERAH\", \"QUEENSTOWN\",\"OUTRAM\", \"ROCHOR\", \"KALLANG\",\"TANGLIN\", \"CLEMENTI\", \"BEDOK\", \"JURONG EAST\", \"GEYLANG\", \"BUKIT TIMAH\",\"NOVENA\", \"TOA PAYOH\",\"TUAS\", \"JURONG WEST\",\"SERANGOON\", \"BISHAN\",\"TAMPINES\", \"BUKIT BATOK\",\"HOUGANG\", \"ANG MO KIO\",\"PASIR RIS\", \"BUKIT PANJANG\", \"YISHUN\", \"PUNGGOL\",\"CHOA CHU KANG\", \"SENGKANG\",\"SEMBAWANG\", \"WOODLANDS\")\narea_names <- gsub(\" \", \"_\", area_names)\n\n\n\n\n\nChildcareEldercareKindergartenNew HawkerHealthy HawkerThe Rest\n\n\n\n# set directory\nsetwd(\"data/rds/facilities/combined/supermarket\")\n\n# list all .rds files\nfiles <- list.files(pattern = \".rds\")\n\n# loop through files and create separate dataframes\nfor (file in files) {\n  # read in file and create dataframe with unique name\n  df_name <- sub(\".rds\", \"\", file)\n  assign(df_name, readRDS(file))\n}\n\n\n\n\n# set directory\nsetwd(\"data/rds/facilities/combined/eldercare\")\n\n# list all .rds files\nfiles <- list.files(pattern = \".rds\")\n\n# loop through files and create separate dataframes\nfor (file in files) {\n  # read in file and create dataframe with unique name\n  df_name <- sub(\".rds\", \"\", file)\n  assign(df_name, readRDS(file))\n}\n\n\n\n\n# set directory\nsetwd(\"data/rds/facilities/combined/kindergarten\")\n\n# list all .rds files\nfiles <- list.files(pattern = \".rds\")\n\n# loop through files and create separate dataframes\nfor (file in files) {\n  # read in file and create dataframe with unique name\n  df_name <- sub(\".rds\", \"\", file)\n  assign(df_name, readRDS(file))\n}\n\n\n\n\n# set directory\nsetwd(\"data/rds/facilities/combined/shopping_mall\")\n\n# list all .rds files\nfiles <- list.files(pattern = \".rds\")\n\n# loop through files and create separate dataframes\nfor (file in files) {\n  # read in file and create dataframe with unique name\n  df_name <- sub(\".rds\", \"\", file)\n  assign(df_name, readRDS(file))\n}\n\n\n\n\n# set directory\nsetwd(\"data/rds/facilities/combined/hawker_healthy\")\n\n# list all .rds files\nfiles <- list.files(pattern = \".rds\")\n\n# loop through files and create separate dataframes\nfor (file in files) {\n  # read in file and create dataframe with unique name\n  df_name <- sub(\".rds\", \"\", file)\n  assign(df_name, readRDS(file))\n}\n\n\n\n\n# set directory\nsetwd(\"data/rds/point\")\n\n# list all .rds files\nfiles <- list.files(pattern = \".rds\")\n\n# loop through files and create separate dataframes\nfor (file in files) {\n  # read in file and create dataframe with unique name\n  df_name <- sub(\".rds\", \"\", file)\n  assign(df_name, readRDS(file))\n}\n\n\n\n\n\n\n\n\n# set directory\nsetwd(\"data/rds/shape\")\n\n# list all .rds files\nfiles <- list.files(pattern = \".rds\")\n\n# loop through files and create separate dataframes\nfor (file in files) {\n  # read in file and create dataframe with unique name\n  df_name <- sub(\".rds\", \"\", file)\n  assign(df_name, readRDS(file))\n}\n\n\n\n\n\n# set the directory containing the RDS files\ndir_path <- \"data/rds/network\"\n\n# get a list of all RDS files in the directory\nnetwork_files <- list.files(path = dir_path, pattern = \"^network_.*\\\\.rds$\", full.names = TRUE)\n\n# loop through the list of files and read each RDS file into a new variable\nfor (file_path in network_files) {\n  area_name <- gsub(\"^network_(.*?)\\\\.rds$\", \"\\\\1\", basename(file_path))\n  area_name <- gsub(\" \", \"_\", area_name)\n  network <- readRDS(file_path)\n  # Convert geometries to linestrings\n  network$geometry <- st_cast(network$geometry, \"MULTILINESTRING\")\n  assign(paste0(\"network_\", area_name), network)\n}\n\n\n\n\n\n# set directory\nsetwd(\"data/rds/point\")\n\n# list all .rds files\nfiles <- list.files(pattern = \".rds\")\n\n# loop through files and create separate dataframes\nfor (file in files) {\n  # read in file and create dataframe with unique name\n  df_name <- sub(\".rds\", \"\", file)\n  assign(df_name, readRDS(file))\n}"
  },
  {
    "objectID": "code_draft/network_constraint.html#childcare-3",
    "href": "code_draft/network_constraint.html#childcare-3",
    "title": "Network Constrained SP Pattern Analysis",
    "section": "Childcare",
    "text": "Childcare\n\nAng Mo KioBedokBishanBukit BatokBukit MerahBukit PanjangBukit TimahChoa Chu KangClementiHougangJurong EastJurong WestKallangMarine ParadeNovenaOutramParis RisPunggolQueenstownRochorSembawangSengkangSerangoonTampinesTanglinToa PayohTuasWoodlandsYishun\n\n\n\ntmap_mode('view')\ntm_shape(shape_ANG_MO_KIO) +\n  tm_polygons() +\ntm_shape(lixel_childcare_ANG_MO_KIO)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_ANG_MO_KIO)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_BEDOK) +\n  tm_polygons() +\ntm_shape(lixel_childcare_BEDOK)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_BEDOK)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_BISHAN) +\n  tm_polygons() +\ntm_shape(lixel_childcare_BISHAN)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_BISHAN)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_BUKIT_BATOK) +\n  tm_polygons() +\ntm_shape(lixel_childcare_BUKIT_BATOK)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_BUKIT_BATOK)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_BUKIT_MERAH) +\n  tm_polygons() +\ntm_shape(lixel_childcare_BUKIT_MERAH)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_BUKIT_MERAH)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_BUKIT_PANJANG) +\n  tm_polygons() +\ntm_shape(lixel_childcare_BUKIT_PANJANG)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_BUKIT_PANJANG)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_BUKIT_TIMAH) +\n  tm_polygons() +\ntm_shape(lixel_childcare_BUKIT_TIMAH)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_BUKIT_TIMAH)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_CHOA_CHU_KANG) +\n  tm_polygons() +\ntm_shape(lixel_childcare_CHOA_CHU_KANG)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_CHOA_CHU_KANG)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_CLEMENTI) +\n  tm_polygons() +\ntm_shape(lixel_childcare_CLEMENTI)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_CLEMENTI)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_HOUGANG) +\n  tm_polygons() +\ntm_shape(lixel_childcare_HOUGANG)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_HOUGANG)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_JURONG_EAST) +\n  tm_polygons() +\ntm_shape(lixel_childcare_JURONG_EAST)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_JURONG_EAST)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_JURONG_WEST) +\n  tm_polygons() +\ntm_shape(lixel_childcare_JURONG_WEST)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_JURONG_WEST)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_KALLANG) +\n  tm_polygons() +\ntm_shape(lixel_childcare_KALLANG)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_KALLANG)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_MARINE_PARADE) +\n  tm_polygons() +\ntm_shape(lixel_childcare_MARINE_PARADE)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_MARINE_PARADE)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_NOVENA) +\n  tm_polygons() +\ntm_shape(lixel_childcare_NOVENA)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_NOVENA)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_OUTRAM) +\n  tm_polygons() +\ntm_shape(lixel_childcare_OUTRAM)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_OUTRAM)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(15,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_PASIR_RIS) +\n  tm_polygons() +\ntm_shape(lixel_childcare_PASIR_RIS)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_PASIR_RIS)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_PUNGGOL) +\n  tm_polygons() +\ntm_shape(lixel_childcare_PUNGGOL)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_PUNGGOL)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_QUEENSTOWN) +\n  tm_polygons() +\ntm_shape(lixel_childcare_QUEENSTOWN)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_QUEENSTOWN)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_ROCHOR) +\n  tm_polygons() +\ntm_shape(lixel_childcare_ROCHOR)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_ROCHOR)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(15,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_SEMBAWANG) +\n  tm_polygons() +\ntm_shape(lixel_childcare_SEMBAWANG)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_SEMBAWANG)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_SENGKANG) +\n  tm_polygons() +\ntm_shape(lixel_childcare_SENGKANG)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_SENGKANG)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_SERANGOON) +\n  tm_polygons() +\ntm_shape(lixel_childcare_SERANGOON)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_SERANGOON)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_TAMPINES) +\n  tm_polygons() +\ntm_shape(lixel_childcare_TAMPINES)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_TAMPINES)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_TANGLIN) +\n  tm_polygons() +\ntm_shape(lixel_childcare_TANGLIN)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_TANGLIN)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_TOA_PAYOH) +\n  tm_polygons() +\ntm_shape(lixel_childcare_TOA_PAYOH)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_TOA_PAYOH)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_TUAS) +\n  tm_polygons() +\ntm_shape(lixel_childcare_TUAS)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_TUAS)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(13,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_WOODLANDS) +\n  tm_polygons() +\ntm_shape(lixel_childcare_WOODLANDS)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_WOODLANDS)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\ntmap_mode('view')\ntm_shape(shape_YISHUN) +\n  tm_polygons() +\ntm_shape(lixel_childcare_YISHUN)+\n  tm_lines(col=\"density\", lwd=2)+\ntm_shape(hdb_childcare_YISHUN)+\n  tm_dots(col = \"Point Type\") +\ntm_view(set.zoom.limits = c(14,16))"
  },
  {
    "objectID": "code_draft/network_constraint.html#eldercare-5",
    "href": "code_draft/network_constraint.html#eldercare-5",
    "title": "Network Constrained SP Pattern Analysis",
    "section": "Eldercare",
    "text": "Eldercare\n\nAng Mo KioBedokBishanBukit BatokBukit MerahBukit PanjangBukit TimahChoa Chu KangClementiGeylangHougangJurong EastJurong WestKallangMarine ParadeNovenaOutramPasir RisPunggolQueenstownRochorSembawangSengkangSerangoonTampinesTanglinToa PayohTuasWoodlandsYishun\n\n\n\n#LINESTRING\nkfunc_eldercare_ANG_MO_KIO$plotk\n\n\n#MULTILINESTRING\nkfunc_eldercare_ANG_MO_KIO$plotk\n\n\n\n\nkfunc_eldercare_BEDOK$plotk\n\n\n\n\nkfunc_eldercare_BISHAN$plotk\n\n\n\n\n#LINESTRING\nkfunc_eldercare_BUKIT_BATOK$plotk\n\n\n#MULTILINESTRING\nkfunc_eldercare_BUKIT_BATOK$plotk\n\n\n\n\nkfunc_eldercare_BUKIT_MERAH$plotk\n\n\n\n\nkfunc_eldercare_BUKIT_PANJANG$plotk\n\n\n\n\nkfunc_eldercare_BUKIT_TIMAH$plotk\n\n\n\n\nkfunc_eldercare_CHOA_CHU_KANG$plotk\n\n\n\n\nkfunc_eldercare_CLEMENTI$plotk\n\n\n\n\nkfunc_eldercare_GEYLANG$plotk\n\n\n\n\nkfunc_eldercare_HOUGANG$plotk\n\n\n\n\nkfunc_eldercare_JURONG_EAST$plotk\n\n\n\n\nkfunc_eldercare_JURONG_WEST$plotk\n\n\n\n\n#LINESTRING\nkfunc_eldercare_KALLANG$plotk\n\n\n#MULTILINESTRING\nkfunc_eldercare_KALLANG$plotk\n\n\n\n\n#LINESTRING\nkfunc_eldercare_MARINE_PARADE$plotk\n\n\n#MULTILINESTRING\nkfunc_eldercare_MARINE_PARADE$plotk\n\n\n\n\n#LINESTRING\nkfunc_eldercare_NOVENA$plotk\n\n\n#MULTILINESTRING\nkfunc_eldercare_NOVENA$plotk\n\n\n\n\n#LINESTRING\nkfunc_eldercare_OUTRAM$plotk\n\n\n#MULTILINESTRING\nkfunc_eldercare_OUTRAM$plotk\n\n\n\n\nkfunc_eldercare_PASIR_RIS$plotk\n\n\n\n\nkfunc_eldercare_PUNGGOL$plotk\n\n\n\n\nkfunc_eldercare_QUEENSTOWN$plotk\n\n\n\n\n#LINESTRING\nkfunc_eldercare_ROCHOR$plotk\n\n\n#MULTILINESTRING\nkfunc_eldercare_ROCHOR$plotk\n\n\n\n\nkfunc_eldercare_SEMBAWANG$plotk\n\n\n\n\nkfunc_eldercare_SENGKANG$plotk\n\n\n\n\n#LINESTRING\nkfunc_eldercare_SERANGOON$plotk\n\n\n#MULTILINESTRING\nkfunc_eldercare_SERANGOON$plotk\n\n\n\n\nkfunc_eldercare_TAMPINES$plotk\n\n\n\n\n#LINESTRING\nkfunc_eldercare_TANGLIN$plotk\n\n\n#MULTILINESTRING\nkfunc_eldercare_TANGLIN$plotk\n\n\n\n\nkfunc_eldercare_TOA_PAYOH$plotk\n\n\n\n\n#LINESTRING\nkfunc_eldercare_TUAS$plotk\n\n\n#MULTILINESTRING\nkfunc_eldercare_TUAS$plotk\n\n\n\n\nkfunc_eldercare_WOODLANDS$plotk\n\n\n\n\nkfunc_eldercare_YISHUN$plotk"
  },
  {
    "objectID": "code_draft/network_constraint.html#to-copy-and-paste",
    "href": "code_draft/network_constraint.html#to-copy-and-paste",
    "title": "Network Constrained SP Pattern Analysis",
    "section": "TO COPY AND PASTE",
    "text": "TO COPY AND PASTE\n\nAng Mo KioBedokBishanBukit BatokBukit MerahBukit PanjangBukit TimahChoa Chu KangClementiGeylangHougangJurong EastJurong WestKallangMarine ParadeNovenaOutramPasir RisPunggolQueenstownRochorSembawangSengkangSerangoonTampinesTanglinToa PayohTuasWoodlandsYishun"
  },
  {
    "objectID": "code_draft/plot_lixel.html",
    "href": "code_draft/plot_lixel.html",
    "title": "Plotting Lixel",
    "section": "",
    "text": "For plotting lixel, will require:"
  },
  {
    "objectID": "code_draft/plot_lixel.html#shape",
    "href": "code_draft/plot_lixel.html#shape",
    "title": "Plotting Lixel",
    "section": "Shape",
    "text": "Shape\n\n# set directory\nsetwd(\"data/rds/shape\")\n\n# list all .rds files\nfiles <- list.files(pattern = \".rds\")\n\n# loop through files and create separate dataframes\nfor (file in files) {\n  # read in file and create dataframe with unique name\n  df_name <- sub(\".rds\", \"\", file)\n  assign(df_name, readRDS(file))\n}"
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Project Deliverable",
    "section": "",
    "text": "1. Project Webpage\n\n\n2. Research Paper\n\n\n3. R Shiny Application\n\n\n4. User Guide R Shiny Application\n\n\n5. Poster"
  },
  {
    "objectID": "project_proposal.html#project-motivation",
    "href": "project_proposal.html#project-motivation",
    "title": "G1T7 Project Research Paper",
    "section": "Project Motivation",
    "text": "Project Motivation\nMany prospective HDB buyers face challenges in visualising and understanding the amenities and facilities available in the vicinity of their desired location. They often resort to manual searches on platforms like Google, which can be time-consuming and frustrating. Moreover, current mapping tools do not offer a comprehensive and tailored view of amenities specific to HDBs.\nOur motivation is to simplify this process by providing a user-friendly analytical app that enables HDB buyers to view and understand the surrounding amenities easily. Our app utilizes advanced spatial point pattern analysis techniques to visualize the distribution and clustering of amenities relevant to HDB buyers. By providing a customized view of amenities that cater specifically to the needs of HDB buyers, we aim to empower HDB buyers to make more informed decisions about their purchases and feel more confident in their chosen residence."
  },
  {
    "objectID": "project_proposal.html#our-methodology-observations-and-problems-faced",
    "href": "project_proposal.html#our-methodology-observations-and-problems-faced",
    "title": "G1T7 Project Research Paper",
    "section": "Our Methodology, Observations and Problems Faced",
    "text": "Our Methodology, Observations and Problems Faced\n\n1. Data Collection\nIn the initial stages of our project, we collected a range of datasets to analyze the spatial point patterns of HDB locations and amenities that we believe would be valuable to HDB buyers. These datasets comprise of various amenities such as MRT stations, bus stops, supermarkets, hawker centres, retail pharmacies, carparks, and more. We acquired some of these datasets online, while others were extracted using OneMap API.\n\n1.1 List of Datasets Used\nBelow is a table of the data sets that we will be using for our project.\n\n\n\nName\nDescription\nFile Format\n\n\n\n\nHDB Resale Flat Prices\nProvides HDB addresses, blocks and street name | Data.gov.sg\n.csv\n\n\nSchool Directory and Information\nProvides a list of primary schools in Singapore | Data.gov.sg\n.csv\n\n\nShopping Malls\nProvides a list of shopping mall and its geometry in Singapore | Web scrapped shopping mall data in 2019 by Valery Lim\n.csv\n\n\nURA Master Plan 2019 Subzone Boundary\nProvides region boundary data | Referenced/taken from Prof Kam\nshp\n\n\nBus Stops\nProvides a list of bus stops and its geometry in Singapore | Datamall LTA\n.shp\n\n\nTrain Station\nProvides a list of MRT/LTR exits and its geometry in Singapore | Datamall LTA\n.shp\n\n\nSupermarkets\nProvides a list of supermarkets in Singapore | Data.gov.sg\n.geojson\n\n\nChildcare Centres\nProvides a list of names, addresses and relevant information for childcare centres in Singapore | Extracted via onemapAPI API Docs | Registration\n.rds\n\n\nEldercare Centres\nProvides a list of names, addresses and relevant information for eldercare centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nKindergartens\nProvides a list of names, addresses and relevant information for kindergartens in Singapore | Extracted via onemapAPI\n.rds\n\n\nHawker Centres\nProvides a list of names, addresses and relevant information for hawker centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nHealthier Hawker Centres\nProvides a list of names, addresses and relevant information for healthier hawker centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nNational Parks\nProvides a list of names, addresses and relevant information for national parks in Singapore | Extracted via onemapAPI\n.rds\n\n\nGyms\nProvides a list of names, addresses and relevant information for gyms in Singapore | Extracted via onemapAPI\n.rds\n\n\nRetail Pharmacies\nProvides a list of names, addresses and relevant information for retail pharmacies in Singapore | Extracted via onemapAPI\n.rds\n\n\nSingapore Police Force (SPF) Establishments\nProvides a list of names, addresses and relevant information for SPF establishments in Singapore | Extracted via onemapAPI\n.rds\n\n\nCarparks\nProvides a list of names, addresses and relevant information for carparks in Singapore | Extracted via onemapAPI\n.rds\n\n\nLibraries\nProvides a list of names, addresses and relevant information for libraries in Singapore | Extracted via onemapAPI | To be used as a sample dataset to test the upload dataset button in our Shiny application\n.rds\n\n\n\n\n\n1.2 Packages Used\nBelow is the list of packages we will be using for our project:\n\nshiny: to create the Shiny application\nshinyjs: to perform JavaScript operations on the Shiny application\nshinyWidget: to customise input widgets on the Shiny application\nleaflet: to create interactive map on the Shiny application\nbslib: to create a modern UI interface for the Shiny application using Bootstrap\nrvest: to parse HTML and XML files\njsonlite: to parse and generate JSON files\nsf: to import and handle geospatial data\nsp: to import and handle spatial point and lines data frames\nrdgal: to import geospatial data and store them as sp objects\nreadxl: to read and import excel files\ntidyverse: to handle data wrangling (tidyr, dplyr, ggplot2, tibble)\nmaptools: to manipulate geographic data\ndata.table: to speed up the modification of data\nhttr: to run API requests (for getting data from onemapsg)\ntmap: to plot out choropleth maps\nggplot2: to create elegant data visualisation for mapping\nggpubr: to customise ggplots for better visualisation\nggthemes: to add preset themes, geoms and scales for ggplot2\nplotly: to create interactive web graphics from ggplot2 graphs\nfunModeling: to plot EDA with better visualisation\nraster: to convert grid output to raster layer for visualisation (Kernel Density Estimation)\nspatstat: to convert spatial objects to ppp format (Spatial Point Pattern plot)\nolsrr: to build OLS regression models\nsfdep: to find contiguous neighbours, calculate weights and perform LISAs (Local Indicator of Spatial Association)\nspNetwork: to perform spatial analysis on networks. Includes network kernel density estimations as well as K function estimations for point pattern analysis on the network.\n\n\n\n\n\n2. Initial Data Pre-processing\nTo optimize the loading speed of the application, we carefully considered the size of the HDB location dataset (i.e HDB Resale Flat Prices), as its large volume may lead to slow loading times. Consequently, we extracted the HDB locations of 5-room flat types over a one-year period from January 2022 to December 2022. We then preprocessed all the datasets by extracting relevant columns, removing missing values, and ensuring that the geometries are valid and have the correct coordinate reference system (CRS) information, specifically EPSG:3414. Afterwards, we carried out our spatial point analysis mentioned under our objectives.\n\n\n3. Spatial Point Analysis\n\n3.1 Measure the level of spatial association and heterogeneity between HDB locations and surrounding amenities using Colocation Quotients (CLQs) analysis\n\n3.1.1 Data Preparation\nBefore we can perform CLQ, we need to extract the datasets to contain the type of amenity with its respective geometry without the address names of the amenities. For example, the code chunk below extracts out the geometry of the HDB locations and names it as type \"HDB\".\n\n\nShow the code\nhdb_a <- resale_sf %>%\n  select(geometry) %>% \n  mutate(geometry= geometry,\n         type = \"HDB\") \n\n\n\n\n3.1.2 Computing Colocation Quotients (CLQs)\nTo streamline the process of computing Colocation Quotients (CLQs) for multiple amenities, a function called cql_func(), which takes in the HDB extracted dataset, a sf dataframe of the amenity that has yet to be extracted and the name of the amenity, was created to extract the relevant datasets, perform the necessary computations, and save the output to an rds file. This enables easy import and visualization of the results in the Shiny app. Below are the steps of the function created:\n\nCreate the extracted dataset for the amenity\nCombine HDB and Amenity dataset into a single dataframe\nPrepare nearest neighbours list by using st_knn() of sfdep package to determine the k (i.e. 6) nearest neighbours for given point geometry.\nCompute kernel weights by using st_kernel_weights() of sfdep package to derive a weights list by using a kernel function.\nPrepare a vector list to reference the points data. For example, A list will be the HDB location points and B list will be the location points of another amenity such as Hawker Centres\nCompute the LCLQ values for each point event of HDB points using local_colocation() \nJoin the output of local_colocation() to the previous HDB points and amenities data.frame\n\n\nBelow is the code chunk for the function created.\n\n\nShow the code\ncql_func <- function(df1,df2,nameB){\n  df2_b <- df2 %>%\n  select(geometry) %>% \n  mutate(geometry= geometry,\n         type = nameB) \n  combined <- rbindlist(list(df1, df2_b),use.names=FALSE)\n  combined_sf <- st_as_sf(combined) %>%\n  st_transform(crs = 3414)\n  \n  nb <- include_self(\n  st_knn(st_geometry(combined_sf), 6))\n  \n  wt <- st_kernel_weights(nb, \n                        combined_sf, \n                        \"gaussian\", \n                        adaptive = TRUE)\n  \n  hdb_locations <- combined_sf %>%\n  filter(type == \"HDB\")\n  A <- hdb_locations$type\n  \n  df2_locations <- combined_sf %>%\n  filter(type == nameB)\n  \n  B <- df2_locations$type\n  \n  LCLQ <- local_colocation(A, B, wt, nb, 99) # 100 simulations\n  LCLQ_stores <- cbind(combined_sf, LCLQ)\n  \n  write_rds(LCLQ_stores, paste0(\"data/rds/clq/hdb_\",nameB,\".rds\"))\n  }\n\n\n\nWhen visualizing the results in the Shiny app, I applied a filter to display only the p-values that are less than 0.05. This allows users to easily identify the significant associations between HDB locations and amenities. The code chunk can be referred to below.\n\n\nShow the code\n clq_map <- tm_shape(mpsz_sf) +\n      tm_polygons() +\n      tm_shape(result, subset = result$amenity_pvalue < 0.05) + #filter out p_value less than 0.05 \n      tm_dots(col = amenity_name,\n              size = 0.01,\n              border.col = \"black\",\n              border.lwd = 0.5) +\n      tm_view(set.zoom.limits = c(11, 14))\n    \n    output$clq_outputmap <- renderLeaflet(tmap_leaflet(clq_map))\n\n\n\n\n3.1.3 Observations\nBy looking at the LCLQ map for HDB locations and Hawker centres below, we can observe that there are not many missing values and the local colocation quotient of 0.992. This informs users that the points are isolated but significant as the local colocation quotient is less than 1 with a p-value less than 0.05. The information provided about the LCLQ map for HDB locations and Hawker centres can be useful for HDB buyers in several ways. Firstly, it suggests that there is a high likelihood that HDB flats in the area are located near hawker centres. This can be important for buyers who prioritise having access to food options and dining facilities nearby. Secondly, the fact that the local colocation quotient is close to 1 and the p-value is less than 0.05 indicates that the spatial relationship between HDB locations and hawker centres is not likely to have occurred by chance. This could be interpreted as evidence of intentional planning or zoning by the authorities, which may indicate the area has been designed to meet the needs of residents.\n\n\n\n\nFigure 1: CLQ Map of Hawker Centres in relation to HDB\n\n\n\n\n\n3.2 Use Kernel Density Estimation (KDE) to estimate the intensity of HDB locations in different study areas of Singapore, such as Tampines and Bedok\n\n3.2.1 Data Preparation\nBefore we can perform KDE, we need to first ensure that our sf dataframes for all the amenities type are converted into sp’s Spatial* class, then converted to the Spatial* class into generic sp format, then converted into spatstat’s ppp format and finally converted into an owin object. The code chunk below is an example of us converting the subzone and HDB dataset into an owin object.\n\n\nShow the code\nmpsz <- as_Spatial(mpsz_sf)\nmpsz_sp <- as(mpsz, \"SpatialPolygons\")\nmpsz_owin <- as(mpsz_sp, \"owin\")\n\nresale <- as_Spatial(resale_sf)\nresale_sp <- as(resale, \"SpatialPoints\")\nresale_ppp <- as(resale_sp, \"ppp\")\nresaleSG_ppp = resale_ppp[mpsz_owin]\n\n\nBut before we converted it into an owin, we also made sure to check for any duplicated values and dealt with it by implementing the jittering approach, which adds a small perturbation to the duplicate points so that they do not occupy the exact same space.\n\n\nShow the code\nany(duplicated(resale_ppp))\nmultiplicity(resale_ppp)\nsum(multiplicity(resale_ppp) > 1)\nresale_ppp_jit <- rjitter(resale_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\nFrom there onwards, we rescaled the unit of measurement from metre to kilometre using the rescale() function and saved the file into an rds file so we can visualise it in our application. The code chunk below shows an example of us rescaling the spf, mrt, gyms and HDB datasets and writing it into an rds file.\n\n\nShow the code\nspfSG_ppp.km <- rescale(spfSG_ppp, 1000, \"km\")\nmrtSG_ppp.km <- rescale(mrtSG_ppp, 1000, \"km\")\ngymsSG_ppp.km <- rescale(gymsSG_ppp, 1000, \"km\")\nresaleSG_ppp.km <- rescale(resaleSG_ppp, 1000, \"km\")\nsaveRDS(spfSG_ppp.km, \"data/rds/kde/spfSG_ppp.km.rds\")\nsaveRDS(mrtSG_ppp.km, \"data/rds/kde/mrtSG_ppp.km.rds\")\nsaveRDS(gymsSG_ppp.km, \"data/rds/kde/gymsSG_ppp.km.rds\")\nsaveRDS(resaleSG_ppp.km, \"data/rds/kde/hdbSG_ppp.km.rds\")\n\n\n\n\n3.2.2 Computing Kernel Density Estimation (KDE)\nThe following steps are taken to compute the KDE:\n\nRead the rds file of the amenity chosen by user\nCompute the KDE based on the bandwidth and kernel type selected by the user\nRender the KDE plot in the application\n\nThe code chunk below shows how we can compute and plot the KDE.\n\n\nShow the code\nindex_kde <- match(input$kde_amenity, names(kde_files))\n    kde_name <- kde_files[index_kde]\n    kde_rds <- readRDS(paste0(\"data/rds/kde/\",kde_name,\".rds\"))\n    \n    \n# Calculate kernel density estimate\nkde_result <- density(kde_rds, \n                          sigma = get(input$kde_bw), \n                          edge = TRUE, \n                          kernel = input$kde_kernel)\n    \n# Render plot\noutput$kde_plot <- renderPlot({\n      plot(kde_result) \n  })\n\n\nBased on the KDE plot, users will be able to identify high or low concentrations of the amenity type. The lighter the color of the cluster, the higher concentration of the amenity type.\n\n\nObservations\nBased on the graph below, we used the spatstat package’s density() function and the Gaussian kernel with the bw.diggle() automatic bandwidth selection method to generate a kernel density estimation (KDE) plot for HDB flats. Specifically, we observed a higher concentration of HDB flats in the northeast side of Singapore compared to other regions. This information may be useful for potential buyers who prefer to live in areas with fewer HDB flats to avoid the northeast side of Singapore and focus their property search on other regions.\n\n\n\nFigure 2: KDE(bw.diggle(), Gaussian kernel) Map of HDB flats\n\n\nIn our application, we also allow users to select the bandwidth type and kernel used to compute the KDE. This allows them to customise the type of KDE graph they would like to view.\n\n\n\nFigure 3: KDE(bw.scott(), Quartic kernel) Map of HDB flats\n\n\n\n\n\n3.3 Assess the non-randomness of HDB location distribution in selected study areas, such as Tampines and Bedok, using F-function and Ripley’s L-function.\n\n3.3.1 Data Preparation\nFor the F-function and Ripley’s L-function, we are able to reuse the rds files for the HDB locations and subzone dataset that we have prepared for the KDE. The only difference is that we will be extracting HDB locations that are within a specific area such as Bedok, Tampines, etc. The code chunk below is an example of extracting HDB locations that are within Bedok.\n\n\nShow the code\nbedok = mpsz[mpsz@data$PLN_AREA_N == input$ffunc,]\n    bedok_sp = as(bedok, \"SpatialPolygons\")\n    bedok_owin = as(bedok_sp, \"owin\")\n    resale_bedok_ppp = resale_ppp_jit[bedok_owin]\n\n\n\n\n3.3.2 Computing F-function and Ripley’s L-function\nNext, we can compute an estimation of the F-function and L-function, perform a complete spatial randomness(CSR) test, where the null hypothesis is that the distribution of HDB locations in the user’s chosen area is random, and the alternative hypothesis is that the distribution of HDB locations in the user’s chosen area is non-random and plot the F-function and L-function graph to visualize the results of the analysis.\nThe following code chunk demonstrates how we computed the estimation of the F-function and L-function. Since the F-function is based on random simulation, we set the random seed to 123 to ensure the reproducibility of the analysis. Additionally, we perform a test at a significance level of 0.05, so the value of alpha will be 0.05 and nsim(the number of simulated point patterns to be generated when computing the envelope.) will be 39.\n\n\nShow the code\nset.seed(123)\nF_bedok.csr <- envelope(resale_bedok_ppp, Fest, nsim = 39)\n L_bedok.csr <- envelope(resale_bedok_ppp, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\n\nBased on the resulting plots, users will be able to observe the F-function or L-function (a solid black line) and the envelope (shaded area), which represents the range of values that would be expected under CSR test. If the observed F-function or L-function lies outside the envelope, it suggests that the data depart significantly from the CSR test, indicating either clustering or regularity in the point pattern. If the observed F-function or L-function lies inside the envelope, it suggests that the data are consistent with CSR, meaning that the pattern of the point locations is random, and no significant spatial clustering or regularity is present in the data.\n\n\n3.3.3 Observations\nBelow is an example of the F-function graph of the Bedok area. We can observed that the F-function and L-function lies outside the envelope, this suggests that the HDB data depart significantly from CSR test.\n\n\n\nFigure 4: F-function graph for HDB flats in Bedok\n\n\n\n\n\nFigure 5: L-function graph for HDB flats in Bedok\n\n\n\n\n\n3.4 Apply Network Constrained Spatial Point Patterns Analysis to analyse the spatial distribution of HDB flats over a street network.\nLastly, we conducted a network constrained spatial point pattern analysis on our datasets, using the package spNetwork. We performed a network KDE as well as the K-Function analysis.\n\n3.4.1 Data Preparation\nFirst, we split all our data sets by the planning area (“PLN_AREA_N”). This included the HDB points, facilities points, mpsz as well as the road network. There are only 30 planning areas that contain HDB points, we will only be using these 30 areas.\n\n3.4.1.1 Splitting mpsz\nWe split the multipolygon shapefile mpsz_sf into smaller subsets by the PLN_AREA_N (planning area name). We then saved each subset named “shape_PLN_AREA_N.rds” according to the planning area.\n\n\nShow the code\n# splitting mpsz by PLN_AREA_N\nfor (x in unique(mpsz_sf$PLN_AREA_N)) {\n  \n  # filter the polygons data frame to get only the polygons for the current area\n  area_polygons <- mpsz_sf %>%\n    filter(PLN_AREA_N == x) %>%\n    st_as_sf() # convert to sf object if necessary\n  \n  # replace spaces with underscores in the area name\n  area_name_underscore <- gsub(\" \", \"_\", x)\n  \n  # construct the name of the output file\n  output_filename <- paste0(\"shape_\", area_name_underscore, \".rds\")\n  \n  # save the area polygons as an RDS file\n  write_rds(area_polygons, file = output_filename)\n}\n\n\n\n\n3.4.1.2 Splitting Road Networks\nNext, we split the road networks based on the intersection of the road networks and the planning areas. We used the st_intersection() function and then saved each intersection as an rds file named “network_PLN_AREA_N.rds” according to the planning area.\n\n\nShow the code\ngrouped_polygons <- mpsz_sf %>%\n  group_by(PLN_AREA_N) %>%\n  summarize(geometry = st_combine(geometry)) %>%\n  st_as_sf()\nintersected <- st_intersection(road_network_lines, grouped_polygons)\nfor (PLN_AREA_N in unique(intersected$PLN_AREA_N)) {\n  filtered <- intersected[intersected$PLN_AREA_N == PLN_AREA_N,]\n  write_rds(filtered, paste0(\"data/geospatial/network/network_\", PLN_AREA_N, \".rds\"))\n}\n\n\n\n\n3.4.1.3 Splitting and Combining HDB and Facility Points\nLooping through the planning areas, we first find the HDB spatial points and facility spatial points respectively in each planning area of the mpsz multipolygon using the st_intersection() function. We selected the geometry column and added a column “Point Type” labelling each row as either “HDB” or “Facility Name”. We combined the HDB and facility spatial points into a list then wrote it as a rds file named “hdb_facility_PLN_AREA_N.rds” for easy future importation.\n\n\nShow the code\n# initialise an empty list to store the output dataframes\noutput_dfs <- list()\n\n# loop over the unique area_names for each multipolygon\nfor (x in unique(mpsz_sf$PLN_AREA_N)) {\n  # subset the multipolygon dataframe by area_name\n  area_multipolygons <- mpsz_sf[mpsz_sf$PLN_AREA_N == x,]\n  # perform a spatial join between the points1 and area_multipolygons dataframes\n  points1_in_polygons <- st_intersection(hdb_points, area_multipolygons)\n  points1_in_polygons <- points1_in_polygons %>%\n    select(c(\"geometry\")) %>%\n    mutate(`Point Type` = \"HDB\") %>%\n    relocate(`Point Type`, .before = 1)\n  \n  # perform a spatial join between the facility points and area_multipolygons dataframes\n  points2_in_polygons <- st_intersection(facility_sf, area_multipolygons)\n  points2_in_polygons <- points2_in_polygons %>%\n    select(c(\"geometry\")) %>%\n    mutate(`Point Type` = \"Facility Name\") %>%\n    relocate(`Point Type`, .before = 1)\n  # create a unique name for the output dataframe\n  output_name <- paste0(\"hdb_facility_\", gsub(\" \", \"_\", x))\n  # store the output data frames in the output_dfs list\n  output_dfs[[output_name]] <- bind_rows(points1_in_polygons, points2_in_polygons)\n}\n\n# write the output data frames to RDS files\n\nfor (i in seq_along(output_dfs)) {\n  filename <- gsub(\" \", \"_\", names(output_dfs)[i]) # replace spaces with underscore\n  write_rds(output_dfs[[i]], paste0(filename, \".rds\")) # save each data frame as an RDS file\n}\n\n\n\n\n\n3.4.2 NetKDE Analysis\nIn earlier steps while splitting the HDB spatial points by the planning area, we realised that out of the 55 planning areas, only 30 planning areas include HDB spatial points. The area names are as shown below:\n\n\nShow the code\n# area names\narea_names <- c(\"MARINE PARADE\", \"BUKIT MERAH\", \"QUEENSTOWN\",\"OUTRAM\", \"ROCHOR\", \"KALLANG\",\"TANGLIN\", \"CLEMENTI\", \"BEDOK\", \"JURONG EAST\", \"GEYLANG\", \"BUKIT TIMAH\",\"NOVENA\", \"TOA PAYOH\",\"TUAS\", \"JURONG WEST\",\"SERANGOON\", \"BISHAN\",\"TAMPINES\", \"BUKIT BATOK\",\"HOUGANG\", \"ANG MO KIO\",\"PASIR RIS\", \"BUKIT PANJANG\", \"YISHUN\", \"PUNGGOL\",\"CHOA CHU KANG\", \"SENGKANG\",\"SEMBAWANG\", \"WOODLANDS\")\narea_names <- gsub(\" \", \"_\", area_names)\n\n\n\n3.4.2.1 Performing the Analysis\nSince our main focus is on HDB points, we will only cover these 30 planning areas moving forward for the analysis. We took reference to “R for Geospatial Data Science and Analytics” to perform the NetKDE analysis below.\nSince there are a lot of points and networks to run through the analysis, running it on loop will result in the immediate crashing of RStudio. Thus we had to go through each planning area and facility manually by editing the facility and area name ‘n’ below.\n\n\nShow the code\nsetwd(\"data/rds/lixel/facility\")\n\n# get data\narea_name <- area_names[n]\n\nhdb_facility <- get(paste0(\"hdb_facility_\", area_name))\nhdb_facility_points <- hdb_facility %>% select(c(\"geometry\"))\narea_network <- get(paste0(\"network_\", area_name))\n\n# prepare lixel objects\nlixels <- lixelize_lines(area_network, 700, mindist = 350)\n\n# generate line centre points\nsamples <- lines_center(lixels)\n\n# performing NetKDE\n\ndensities <- nkde(area_network,\n                  events = hdb_facility_points,\n                  w = rep(1,nrow(hdb_facility_points)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 100, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n# add densities to samples and lixels\nsamples$density <- densities\nlixels$density <- densities\n\n# rescale densities to help with mapping\nsamples$density <- samples$density * 1000\nlixels$density <- lixels$density * 1000\n\n# save lixel as RDS with unique name\nlixel_name <- paste0(\"lixel_facility_\", area_name, \".rds\")\nsaveRDS(lixels, lixel_name)\n\n\nWe saved each lixel as “lixel_facility_area_name.rds” according to the facility and area names.\n\n\n3.4.2.2 Plotting out Lixels\nUsing the code below, we plotted out the road network density using the lixel generated earlier. We then plotted the HDB and facility points, colour-coding the points for easier visualisation.\n\n\nShow the code\ntmap_mode('view')\n\ntm_shape(shape_area_name) +\n\n  tm_polygons() +\n\ntm_shape(lixel_facility_area_name)+\n\n  tm_lines(col=\"density\", lwd=2)+\n\ntm_shape(hdb_facility_area_name)+\n\n  tm_dots(col = \"Point Type\", palette=c('blue', 'red')) +\n\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\n3.4.2.3 Observations\nOne thing we can observe is the stark difference between the ratio of the number of HDB points to the facility points. Since we have done our analysis based on the planning area, the number of HDB and facility points per planning area is rather small. Thus, the small change in number of facility points and their geometry location greatly affects the density results of our network KDE analysis.\nOur analysis showed that Punggol and Sembawang have the highest network density, followed by Choa Chu Kang, Bukit Batok, and Yishun. These regions are located in the north(-east) and west of Singapore. Conversely, the least dense network was observed in Tuas, which had only three HDB points. Overall, this information can be valuable for potential HDB buyers looking to make informed decisions based on their preferences and needs.\nHowever that being said, areas with little to no facility points gave us less insightful results as it is basically a network KDE analysis on the HDB points itself rather than that of both HDB and facility points. This is a rather recurring observation thus we decided to focus just on the HDB points without the facilities for the next test, K-function.\n\n\n\n3.4.3 K-Function Analysis\n\n3.4.3.1 Performing K-Function\nSimilarly, with reference to “R for Geospatial Data Science and Analytics”, we performed the K-function test for each planning area individually by changing the n for “area_names” as shown below.\n\n\nShow the code\n# set directory to save kfunc files\nsetwd(\"data/rds/kfunc\")\n\n# get data\narea_name <- area_names[n]\n\nhdb_facility <- get(paste0(\"point_\", area_name))\nhdb_facility_points <- hdb_facility %>% select(c(\"geometry\"))\narea_network <- get(paste0(\"network_\", area_name))\n\n# kfunc\nkfun <- kfunctions(area_network, \n                             hdb_facility_points,\n                             start = 0, \n                             end = 1000, \n                             step = 50, \n                             width = 50, \n                             nsim = 50, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05)\n\n# save kfunc as RDS with unique name\nkfunc_name <- paste0(\"kfunc_\", area_name, \".rds\")\nsaveRDS(kfun, kfunc_name)\n\n\n\n\n3.4.3.2 Plotting K-Function\nThe code below plots the K-function graph.\n\n\nShow the code\nkfunc_area_name$plotk\n\n\n\n\n\n\n3.4.4 Problems Faced\nAnother reason why we chose to only do the K-function analysis for HDB points rather than both HDB and facility points is because of the huge file size of each rds file saved. After completing the K-Function analysis for 12 facility points for 30 planning areas, we ended up with a total of 8.51GB for file size. We also contemplated if we wanted to save each K-function as a photo instead but loading over 360 photos together with other files we have into our shiny app would take too much time. Lastly, we also had the option to just load the K-function on the Shiny app itself but this can also take up a lot of time depending on the planning area. Thus,we ended up keeping the 30 K-function analysis that included only HDB points at 30 planning areas.\n\n\n3.4.5 Observations\nWe wanted to focus on the planning areas that we observed were the most and least dense as found out at the NetKDE analysis earlier. The blue line represents the empirical network K-function of the HDB points in the respective planning areas. The grey area represents the results of the 50 simulations in the interval 2.5% - 97.5% .\n\nPunggol:\nThe blue line is a lot higher than the grey area throughout the whole distance (0 to 1000m). Thus, we can interpret that the HDB points are more clustered than what is expected from a random distribution.\n\n\n\nFigure 6: NetKDE K-function of the HDB points in Punggol\n\n\nSembawang:\nThe blue line is a lot higher than the grey area from around 100m to 600m. Thus, we can interpret that the HDB points are more clustered at that distance than what is expected from a random distribution.\n\n\n\n\nFigure 7: NetKDE K-function of the HDB points in Sembawang\n\n\n\nTuas:\nThe blue line is at 0 across the whole distance and there is no grey area. This is in line with the network KDE analysis performed earlier. The HDB points are not clustered because there are only 3 points in such a huge area.\n\n\n\n\nFigure 8: NetKDE K-function of the HDB points in Tuas"
  },
  {
    "objectID": "project_proposal.html#packages-used",
    "href": "project_proposal.html#packages-used",
    "title": "G1T7 Project Research Paper",
    "section": "Packages Used",
    "text": "Packages Used\nBelow is the list of packages we will be using for our project:\n\nshiny: to create the Shiny application\nshinyjs: to perform JavaScript operations on the Shiny application\nshinyWidget: to customise input widgets on the Shiny application\nleaflet: to create interactive map on the Shiny application\nbslib: to create a modern UI interface for the Shiny application using Bootstrap\nrvest: to parse HTML and XML files\njsonlite: to parse and generate JSON files\nsf: to import and handle geospatial data\nsp: to import and handle spatial point and lines data frames\nrdgal: to import geospatial data and store them as sp objects\nreadxl: to read and import excel files\ntidyverse: to handle data wrangling (tidyr, dplyr, ggplot2, tibble)\nmaptools: to manipulate geographic data\ndata.table: to speed up the modification of data\nhttr: to run API requests (for getting data from onemapsg)\ntmap: to plot out choropleth maps\nggplot2: to create elegant data visualisation for mapping\nggpubr: to customise ggplots for better visualisation\nggthemes: to add preset themes, geoms and scales for ggplot2\nplotly: to create interactive web graphics from ggplot2 graphs\nfunModeling: to plot EDA with better visualisation\nraster: to convert grid output to raster layer for visualisation (Kernel Density Estimation)\nspatstat: to convert spatial objects to ppp format (Spatial Point Pattern plot)\nolsrr: to build OLS regression models\nsfdep: to find contiguous neighbours, calculate weights and perform LISAs (Local Indicator of Spatial Association)\nspNetwork: to perform spatial analysis on networks. Includes network kernel density estimations as well as K function estimations for point pattern analysis on the network."
  },
  {
    "objectID": "project_proposal.html#overview-of-our-application-design",
    "href": "project_proposal.html#overview-of-our-application-design",
    "title": "G1T7 Project Research Paper",
    "section": "Overview of Our Application Design",
    "text": "Overview of Our Application Design\nOur Shiny application features a minimalist design, with a clean and modern look achieved through the use of the “minty” bootswatch theme. The layout is simple and intuitive, featuring a navigation bar that provides easy access to various features of the app. Users can explore HDB locations and amenities through a mapping visualization, analyze spatial point patterns with graphs, and even upload their own datasets for further analysis.\nTo provide a better understanding of our application, below are examples of our application screens. The user guide to our application can be found here.\n\nHomepage\nIn this page, it introduces the motivation of our application and the authors.\n\n\n\nFigure 10: Homepage\n\n\nVisualization page\nIn this page, we can observe the mapping of HDB flats and relevant amenities in Singapore.\n\n\n\nFigure 11: Mapping of HDB Flats and Relevant Amenities\n\n\nSpatial Point Analysis\nIn this page, we offer several tools designed to help you analyze the spatial distribution of points in your dataset. Our tools includes the Local Colocation Quotient Analysis (CLQ), Kernel Density Estimation (KDE), F-Function, Ripley K-Function, and Network Constraint Analysis. To access these tools, simply click on the five tabs located at the top of the page. Each tool provides a unique perspective on the spatial patterns in your data, allowing you to gain valuable insights into the underlying processes driving your observations.\n\n\n\nFigure 12: Spatial Point Analysis - CLQ Map\n\n\nData Upload\nIn this page, users can upload their data of an amenity in rds format and it will be shown as an option under the visualisation page, what would you like to view, drop down list.\n\n\n\nFigure 13: Upload Dataset Page"
  },
  {
    "objectID": "project_proposal.html#future-work",
    "href": "project_proposal.html#future-work",
    "title": "G1T7 Project Research Paper",
    "section": "Future Work",
    "text": "Future Work\nTo enhance the usefulness of our app, we can integrate hot and cold spot analysis of amenities to identify areas with a high concentration of amenities, known as hot spots, and areas with a low concentration of amenities, known as cold spots. While our current application uses KDE to estimate the density of amenities across an area, hot and cold spot analysis may offer a more intuitive approach to identifying areas with significant clusters of high or low concentrations. By examining the spatial distribution of amenities, we can pinpoint specific areas and provide more targeted information to HDB buyers. This approach may allow us to deliver more actionable insights and help buyers make more informed decisions about their property search.\nCurrently, our application’s dataset upload function can only upload rds files and is only applicable for visualizations of the map of amenities and HDB locations. To improve the functionality of the app, we can expand the dataset upload function to include other features such as plotting the CLQ maps, KDE, F-function, L-function, and network constraint analysisgraphs. This expansion will allow users to analyze them for deeper insights. Moreover, we can provide additional data pre-processing features to enhance the quality of the dataset uploaded by the user. This ensures that the data is effectively utilized to gain valuable insights and make informed decisions. By offering these advanced features, our application can provide more accurate and relevant information to users and help them make more informed decisions about their property search.\nWe can also enhance our app by integrating it with APIs (Application Programming Interfaces) that provide real-time location data for amenities such as supermarkets, shopping malls, and hawkers in Singapore. By doing so, our app can obtain the most up-to-date information on the availability and locations of these amenities. This integration will ensure that the information provided by our application is accurate and reliable, which is essential for making informed decisions."
  },
  {
    "objectID": "project_proposal.html#our-methodology-and-observations",
    "href": "project_proposal.html#our-methodology-and-observations",
    "title": "G1T7 Project Research Paper",
    "section": "Our Methodology and Observations",
    "text": "Our Methodology and Observations\n\n1. Data Collection\nIn the initial stages of our project, we collected a range of datasets to analyze the spatial point patterns of HDB locations and amenities that we believe would be valuable to HDB buyers. These datasets comprise of various amenities such as MRT stations, bus stops, supermarkets, hawker centres, retail pharmacies, carparks, and more. We acquired some of these datasets online, while others were extracted using OneMap API.\n\n1.1 List of Datasets Used\nBelow is a table of the data sets that we will be using for our project.\n\n\n\nName\nDescription\nFile Format\n\n\n\n\nHDB Resale Flat Prices\nProvides HDB addresses, blocks and street name | Data.gov.sg\n.csv\n\n\nSchool Directory and Information\nProvides a list of primary schools in Singapore | Data.gov.sg\n.csv\n\n\nShopping Malls\nProvides a list of shopping mall and its geometry in Singapore | Web scrapped shopping mall data in 2019 by Valery Lim\n.csv\n\n\nURA Master Plan 2019 Subzone Boundary\nProvides region boundary data | Referenced/taken from Prof Kam\nshp\n\n\nBus Stops\nProvides a list of bus stops and its geometry in Singapore | Datamall LTA\n.shp\n\n\nTrain Station\nProvides a list of MRT/LTR exits and its geometry in Singapore | Datamall LTA\n.shp\n\n\nSupermarkets\nProvides a list of supermarkets in Singapore | Data.gov.sg\n.geojson\n\n\nChildcare Centres\nProvides a list of names, addresses and relevant information for childcare centres in Singapore | Extracted via onemapAPI API Docs | Registration\n.rds\n\n\nEldercare Centres\nProvides a list of names, addresses and relevant information for eldercare centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nKindergartens\nProvides a list of names, addresses and relevant information for kindergartens in Singapore | Extracted via onemapAPI\n.rds\n\n\nHawker Centres\nProvides a list of names, addresses and relevant information for hawker centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nHealthier Hawker Centres\nProvides a list of names, addresses and relevant information for healthier hawker centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nNational Parks\nProvides a list of names, addresses and relevant information for national parks in Singapore | Extracted via onemapAPI\n.rds\n\n\nGyms\nProvides a list of names, addresses and relevant information for gyms in Singapore | Extracted via onemapAPI\n.rds\n\n\nRetail Pharmacies\nProvides a list of names, addresses and relevant information for retail pharmacies in Singapore | Extracted via onemapAPI\n.rds\n\n\nSingapore Police Force (SPF) Establishments\nProvides a list of names, addresses and relevant information for SPF establishments in Singapore | Extracted via onemapAPI\n.rds\n\n\nCarparks\nProvides a list of names, addresses and relevant information for carparks in Singapore | Extracted via onemapAPI\n.rds\n\n\nLibraries\nProvides a list of names, addresses and relevant information for libraries in Singapore | Extracted via onemapAPI | To be used as a sample dataset to test the upload dataset button in our Shiny application\n.rds\n\n\n\n\n\n1.2 Packages Used\nBelow is the list of packages we will be using for our project:\n\nshiny: to create the Shiny application\nshinyjs: to perform JavaScript operations on the Shiny application\nshinyWidget: to customise input widgets on the Shiny application\nleaflet: to create interactive map on the Shiny application\nbslib: to create a modern UI interface for the Shiny application using Bootstrap\nrvest: to parse HTML and XML files\njsonlite: to parse and generate JSON files\ndplyr: to simplify the data manipulation tasks such as filtering rows, selecting columns, grouping and summarizing data, and joining multiple datasets\nsf: to import and handle geospatial data\nsp: to import and handle spatial point and lines data frames\nrdgal: to import geospatial data and store them as sp objects\nreadxl: to read and import excel files\ntidyverse: to handle data wrangling (tidyr, dplyr, ggplot2, tibble)\nmaptools: to manipulate geographic data\ndata.table: to speed up the modification of data\nhttr: to run API requests (for getting data from onemapsg)\ntmap: to plot out choropleth maps\nggplot2: to create elegant data visualisation for mapping\nggpubr: to customise ggplots for better visualisation\nggthemes: to add preset themes, geoms and scales for ggplot2\nplotly: to create interactive web graphics from ggplot2 graphs\nfunModeling: to plot EDA with better visualisation\nraster: to convert grid output to raster layer for visualisation (Kernel Density Estimation)\nspatstat: to convert spatial objects to ppp format (Spatial Point Pattern plot)\nolsrr: to build OLS regression models\nsfdep: to find contiguous neighbours, calculate weights and perform LISAs (Local Indicator of Spatial Association)\nspNetwork: to perform spatial analysis on networks. Includes network kernel density estimations as well as K function estimations for point pattern analysis on the network.\nvctrs: provides a framework for working with vectors and arrays in a consistent and efficient way.\n\n\n\n\n\n2. Initial Data Pre-processing\nTo optimize the loading speed of the application, we carefully considered the size of the HDB location dataset (i.e HDB Resale Flat Prices), as its large volume may lead to slow loading times. Consequently, we extracted the HDB locations of 5-room flat types over a one-year period from January 2022 to December 2022. We then preprocessed all the datasets by extracting relevant columns, removing missing values, and ensuring that the geometries are valid and have the correct coordinate reference system (CRS) information, specifically EPSG:3414. Afterwards, we carried out our spatial point analysis mentioned under our objectives.\n\n\n3. Spatial Point Analysis\n\n3.1 Measure the level of spatial association and heterogeneity between HDB locations and surrounding amenities using Colocation Quotients (CLQs) analysis\n\n3.1.1 Data Preparation\nBefore we can perform CLQ, we need to extract the datasets to contain the type of amenity with its respective geometry without the address names of the amenities. For example, the code chunk below extracts out the geometry of the HDB locations and names it as type “HDB”.\n\n\nShow the code\nhdb_a <- resale_sf %>%\n  select(geometry) %>% \n  mutate(geometry= geometry,\n         type = \"HDB\") \n\n\n\n\n3.1.2 Computing Colocation Quotients (CLQs)\nTo streamline the process of computing Colocation Quotients (CLQs) for multiple amenities, a function called cql_func(), which takes in the HDB extracted dataset, a sf dataframe of the amenity that has yet to be extracted and the name of the amenity, was created to extract the relevant datasets, perform the necessary computations, and save the output to an rds file. This enables easy import and visualization of the results in the Shiny app. Below are the steps of the function created:\n\nCreate the extracted dataset for the amenity\nCombine HDB and Amenity dataset into a single dataframe\nPrepare nearest neighbours list by using st_knn() of sfdep package to determine the k (i.e. 6) nearest neighbours for given point geometry.\nCompute kernel weights by using st_kernel_weights() of sfdep package to derive a weights list by using a kernel function.\nPrepare a vector list to reference the points data. For example, A list will be the HDB location points and B list will be the location points of another amenity such as Hawker Centres\nCompute the LCLQ values for each point event of HDB points using local_colocation() \nJoin the output of local_colocation() to the previous HDB points and amenities data.frame and write it into an rds file\n\nBelow is the code chunk for the function created.\n\n\nShow the code\ncql_func <- function(df1,df2,nameB){\n  df2_b <- df2 %>%\n  select(geometry) %>% \n  mutate(geometry= geometry,\n         type = nameB) \n  combined <- rbindlist(list(df1, df2_b),use.names=FALSE)\n  combined_sf <- st_as_sf(combined) %>%\n  st_transform(crs = 3414)\n  \n  nb <- include_self(\n  st_knn(st_geometry(combined_sf), 6))\n  \n  wt <- st_kernel_weights(nb, \n                        combined_sf, \n                        \"gaussian\", \n                        adaptive = TRUE)\n  \n  hdb_locations <- combined_sf %>%\n  filter(type == \"HDB\")\n  A <- hdb_locations$type\n  \n  df2_locations <- combined_sf %>%\n  filter(type == nameB)\n  \n  B <- df2_locations$type\n  \n  LCLQ <- local_colocation(A, B, wt, nb, 99) # 100 simulations\n  LCLQ_stores <- cbind(combined_sf, LCLQ)\n  \n  write_rds(LCLQ_stores, paste0(\"data/rds/clq/hdb_\",nameB,\".rds\"))\n  }\n\n\n\nWhen visualizing the results in the Shiny app, I filtered the data to display only the p-values that are less than 0.05. This allows users to easily identify the significant associations between HDB locations and amenities. The code chunk can be referred to below.\n\n\nShow the code\n clq_map <- tm_shape(mpsz_sf) +\n      tm_polygons() +\n      tm_shape(result, subset = result$amenity_pvalue < 0.05) + #filter out p_value less than 0.05 \n      tm_dots(col = amenity_name,\n              size = 0.01,\n              border.col = \"black\",\n              border.lwd = 0.5) +\n      tm_view(set.zoom.limits = c(11, 14))\n    \n    output$clq_outputmap <- renderLeaflet(tmap_leaflet(clq_map))\n\n\n\n\n3.1.3 Observations\nBy looking at the LCLQ map for HDB locations and Hawker centres below, we can observe that there are not many missing values and the local colocation quotient of 0.992. This informs users that the points are isolated but significant as the local colocation quotient is less than 1 with a p-value less than 0.05. The information provided about the LCLQ map for HDB locations and Hawker centres can be useful for HDB buyers in several ways. Firstly, it suggests that there is a high likelihood that HDB flats in the area are located near hawker centres. This can be important for buyers who prioritise having access to food options and dining facilities nearby. Secondly, the fact that the local colocation quotient is close to 1 and the p-value is less than 0.05 indicates that the spatial relationship between HDB locations and hawker centres is not likely to have occurred by chance. This could be interpreted as evidence of intentional planning or zoning by the authorities, which may indicate the area has been designed to meet the needs of residents.\n\n\n\n\nFigure 1: CLQ Map of Hawker Centres in relation to HDB\n\n\n\n\n\n3.2 Use Kernel Density Estimation (KDE) to estimate the intensity of HDB locations in different study areas of Singapore, such as Tampines and Bedok\n\n3.2.1 Data Preparation\nBefore we can perform KDE, we need to first ensure that our sf dataframes for all the amenities type are converted into sp’s Spatial* class, then converted to the Spatial* class into generic sp format, then converted into spatstat’s ppp format and finally converted into an owin object. The code chunk below is an example of us converting the subzone and HDB dataset into an owin object.\n\n\nShow the code\nmpsz <- as_Spatial(mpsz_sf)\nmpsz_sp <- as(mpsz, \"SpatialPolygons\")\nmpsz_owin <- as(mpsz_sp, \"owin\")\n\nresale <- as_Spatial(resale_sf)\nresale_sp <- as(resale, \"SpatialPoints\")\nresale_ppp <- as(resale_sp, \"ppp\")\nresaleSG_ppp = resale_ppp[mpsz_owin]\n\n\nBut before we converted it into an owin, we also made sure to check for any duplicated values and dealt with it by implementing the jittering approach, which adds a small perturbation to the duplicate points so that they do not occupy the exact same space.\n\n\nShow the code\nany(duplicated(resale_ppp))\nmultiplicity(resale_ppp)\nsum(multiplicity(resale_ppp) > 1)\nresale_ppp_jit <- rjitter(resale_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\nFrom there onwards, we rescaled the unit of measurement from metre to kilometre using the rescale() function and saved the file into an rds file so we can visualise it in our application. The code chunk below shows an example of us rescaling the spf, mrt, gyms and HDB datasets and writing it into an rds file.\n\n\nShow the code\nspfSG_ppp.km <- rescale(spfSG_ppp, 1000, \"km\")\nmrtSG_ppp.km <- rescale(mrtSG_ppp, 1000, \"km\")\ngymsSG_ppp.km <- rescale(gymsSG_ppp, 1000, \"km\")\nresaleSG_ppp.km <- rescale(resaleSG_ppp, 1000, \"km\")\nsaveRDS(spfSG_ppp.km, \"data/rds/kde/spfSG_ppp.km.rds\")\nsaveRDS(mrtSG_ppp.km, \"data/rds/kde/mrtSG_ppp.km.rds\")\nsaveRDS(gymsSG_ppp.km, \"data/rds/kde/gymsSG_ppp.km.rds\")\nsaveRDS(resaleSG_ppp.km, \"data/rds/kde/hdbSG_ppp.km.rds\")\n\n\n\n\n3.2.2 Computing Kernel Density Estimation (KDE)\nThe following steps are taken to compute the KDE:\n\nRead the rds file of the amenity chosen by user\nCompute the KDE based on the bandwidth and kernel type selected by the user\nRender the KDE plot in the application\n\nThe code chunk below shows how we can compute and plot the KDE.\n\n\nShow the code\nindex_kde <- match(input$kde_amenity, names(kde_files))\n    kde_name <- kde_files[index_kde]\n    kde_rds <- readRDS(paste0(\"data/rds/kde/\",kde_name,\".rds\"))\n    \n    \n# Calculate kernel density estimate\nkde_result <- density(kde_rds, \n                          sigma = get(input$kde_bw), \n                          edge = TRUE, \n                          kernel = input$kde_kernel)\n    \n# Render plot\noutput$kde_plot <- renderPlot({\n      plot(kde_result) \n  })\n\n\nBased on the KDE plot, users will be able to identify high or low concentrations of the amenity type. The lighter the color of the cluster, the higher concentration of the amenity type.\n\n\n3.2.3 Observations\nBased on the graph below, we used the spatstat package’s density() function and the Gaussian kernel with the bw.diggle() automatic bandwidth selection method to generate a kernel density estimation (KDE) plot for HDB flats. Specifically, we observed a higher concentration of HDB flats in the northeast side of Singapore compared to other regions. This information may be useful for potential buyers who prefer to live in areas with fewer HDB flats to avoid the northeast side of Singapore and focus their property search on other regions.\n\n\n\nFigure 2: KDE(bw.diggle(), Gaussian kernel) Map of HDB flats\n\n\nIn our application, we also allow users to select the bandwidth type and kernel used to compute the KDE. This allows them to customise the type of KDE graph they would like to view.\n\n\n\nFigure 3: KDE(bw.scott(), Quartic kernel) Map of HDB flats\n\n\n\n\n\n3.3 Assess the non-randomness of HDB location distribution in selected study areas, such as Tampines and Bedok, using F-function and Ripley’s L-function.\n\n3.3.1 Data Preparation\nFor the F-function and Ripley’s L-function, we are able to reuse the rds files for the HDB locations and subzone dataset that we have prepared for the KDE. The only difference is that we will be extracting HDB locations that are within a specific area such as Bedok, Tampines, etc. The code chunk below is an example of extracting HDB locations that are within Bedok.\n\n\nShow the code\nbedok = mpsz[mpsz@data$PLN_AREA_N == input$ffunc,]\n    bedok_sp = as(bedok, \"SpatialPolygons\")\n    bedok_owin = as(bedok_sp, \"owin\")\n    resale_bedok_ppp = resale_ppp_jit[bedok_owin]\n\n\n\n\n3.3.2 Computing F-function and Ripley’s L-function\nNext, we can compute an estimation of the F-function and L-function, perform a complete spatial randomness(CSR) test, where the null hypothesis is that the distribution of HDB locations in the user’s chosen area is random, and the alternative hypothesis is that the distribution of HDB locations in the user’s chosen area is non-random and plot the F-function and L-function graph to visualize the results of the analysis.\nThe following code chunk demonstrates how we computed the estimation of the F-function and L-function. Since the F-function and L-function is based on a simulation, we set the random seed to 123 to ensure the reproducibility of the analysis. Additionally, we perform a test at a significance level of 0.05, so the value of alpha will be 0.05 and nsim(the number of simulated point patterns to be generated when computing the envelope.) will be 39.\n\n\nShow the code\nset.seed(123)\nF_bedok.csr <- envelope(resale_bedok_ppp, Fest, nsim = 39)\nL_bedok.csr <- envelope(resale_bedok_ppp, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\n\nWith regards to the L-function, it is an interactive map where we referenced the code from our senior, Denise Adele Chua.\nBased on the resulting plots, users will be able to observe the F-function or L-function (a solid black line) and the envelope (shaded area), which represents the range of values that would be expected under CSR test. If the observed F-function or L-function lies outside the envelope, it suggests that the data depart significantly from the CSR test, indicating either clustering or regularity in the point pattern. If the observed F-function or L-function lies inside the envelope, it suggests that the data are consistent with CSR, meaning that the pattern of the point locations is random, and no significant spatial clustering or regularity is present in the data.\n\n\n3.3.3 Observations\nBelow is an example of the F-function graph of the Bedok area. We can observed that the F-function and L-function lies outside the envelope, this suggests that the HDB data depart significantly from CSR test.\n\n\n\nFigure 4: F-function graph for HDB flats in Bedok\n\n\n\n\n\nFigure 5: L-function graph for HDB flats in Bedok\n\n\n\n\n\n3.4 Apply Network Constrained Spatial Point Patterns Analysis to analyse the spatial distribution of HDB flats over a street network.\nLastly, we conducted a network constrained spatial point pattern analysis on our datasets, using the package spNetwork. We performed a network KDE as well as the K-Function analysis.\n\n3.4.1 Data Preparation\nFirst, we split all our data sets by the planning area (“PLN_AREA_N”). This included the HDB points, facilities points, mpsz as well as the road network. There are only 30 planning areas that contain HDB points, we will only be using these 30 areas.\n\n3.4.1.1 Splitting mpsz\nWe split the multipolygon shapefile mpsz_sf into smaller subsets by the PLN_AREA_N (planning area name). We then saved each subset named “shape_PLN_AREA_N.rds” according to the planning area.\n\n\nShow the code\n# splitting mpsz by PLN_AREA_N\nfor (x in unique(mpsz_sf$PLN_AREA_N)) {\n  \n  # filter the polygons data frame to get only the polygons for the current area\n  area_polygons <- mpsz_sf %>%\n    filter(PLN_AREA_N == x) %>%\n    st_as_sf() # convert to sf object if necessary\n  \n  # replace spaces with underscores in the area name\n  area_name_underscore <- gsub(\" \", \"_\", x)\n  \n  # construct the name of the output file\n  output_filename <- paste0(\"shape_\", area_name_underscore, \".rds\")\n  \n  # save the area polygons as an RDS file\n  write_rds(area_polygons, file = output_filename)\n}\n\n\n\n\n3.4.1.2 Splitting Road Networks\nNext, we split the road networks based on the intersection of the road networks and the planning areas. We used the st_intersection() function and then saved each intersection as an rds file named “network_PLN_AREA_N.rds” according to the planning area.\n\n\nShow the code\ngrouped_polygons <- mpsz_sf %>%\n  group_by(PLN_AREA_N) %>%\n  summarize(geometry = st_combine(geometry)) %>%\n  st_as_sf()\nintersected <- st_intersection(road_network_lines, grouped_polygons)\nfor (PLN_AREA_N in unique(intersected$PLN_AREA_N)) {\n  filtered <- intersected[intersected$PLN_AREA_N == PLN_AREA_N,]\n  write_rds(filtered, paste0(\"data/geospatial/network/network_\", PLN_AREA_N, \".rds\"))\n}\n\n\n\n\n3.4.1.3 Splitting and Combining HDB and Facility Points\nLooping through the planning areas, we first find the HDB spatial points and facility spatial points respectively in each planning area of the mpsz multipolygon using the st_intersection() function. We selected the geometry column and added a column “Point Type” labelling each row as either “HDB” or “Facility Name”. We combined the HDB and facility spatial points into a list then wrote it as a rds file named “hdb_facility_PLN_AREA_N.rds” for easy future importation.\n\n\nShow the code\n# initialise an empty list to store the output dataframes\noutput_dfs <- list()\n\n# loop over the unique area_names for each multipolygon\nfor (x in unique(mpsz_sf$PLN_AREA_N)) {\n  # subset the multipolygon dataframe by area_name\n  area_multipolygons <- mpsz_sf[mpsz_sf$PLN_AREA_N == x,]\n  # perform a spatial join between the points1 and area_multipolygons dataframes\n  points1_in_polygons <- st_intersection(hdb_points, area_multipolygons)\n  points1_in_polygons <- points1_in_polygons %>%\n    select(c(\"geometry\")) %>%\n    mutate(`Point Type` = \"HDB\") %>%\n    relocate(`Point Type`, .before = 1)\n  \n  # perform a spatial join between the facility points and area_multipolygons dataframes\n  points2_in_polygons <- st_intersection(facility_sf, area_multipolygons)\n  points2_in_polygons <- points2_in_polygons %>%\n    select(c(\"geometry\")) %>%\n    mutate(`Point Type` = \"Facility Name\") %>%\n    relocate(`Point Type`, .before = 1)\n  # create a unique name for the output dataframe\n  output_name <- paste0(\"hdb_facility_\", gsub(\" \", \"_\", x))\n  # store the output data frames in the output_dfs list\n  output_dfs[[output_name]] <- bind_rows(points1_in_polygons, points2_in_polygons)\n}\n\n# write the output data frames to RDS files\n\nfor (i in seq_along(output_dfs)) {\n  filename <- gsub(\" \", \"_\", names(output_dfs)[i]) # replace spaces with underscore\n  write_rds(output_dfs[[i]], paste0(filename, \".rds\")) # save each data frame as an RDS file\n}\n\n\n\n\n\n3.4.2 NetKDE Analysis\nIn earlier steps while splitting the HDB spatial points by the planning area, we realised that out of the 55 planning areas, only 30 planning areas include HDB spatial points. The area names are as shown below:\n\n\nShow the code\n# area names\narea_names <- c(\"MARINE PARADE\", \"BUKIT MERAH\", \"QUEENSTOWN\",\"OUTRAM\", \"ROCHOR\", \"KALLANG\",\"TANGLIN\", \"CLEMENTI\", \"BEDOK\", \"JURONG EAST\", \"GEYLANG\", \"BUKIT TIMAH\",\"NOVENA\", \"TOA PAYOH\",\"TUAS\", \"JURONG WEST\",\"SERANGOON\", \"BISHAN\",\"TAMPINES\", \"BUKIT BATOK\",\"HOUGANG\", \"ANG MO KIO\",\"PASIR RIS\", \"BUKIT PANJANG\", \"YISHUN\", \"PUNGGOL\",\"CHOA CHU KANG\", \"SENGKANG\",\"SEMBAWANG\", \"WOODLANDS\")\narea_names <- gsub(\" \", \"_\", area_names)\n\n\n\n3.4.2.1 Performing the Analysis\nSince our main focus is on HDB points, we will only cover these 30 planning areas moving forward for the analysis. We took reference to “R for Geospatial Data Science and Analytics” to perform the NetKDE analysis below.\nSince there are a lot of points and networks to run through the analysis, running it on loop will result in the immediate crashing of RStudio. Thus we had to go through each planning area and facility manually by editing the facility and area name ‘n’ below.\n\n\nShow the code\nsetwd(\"data/rds/lixel/facility\")\n\n# get data\narea_name <- area_names[n]\n\nhdb_facility <- get(paste0(\"hdb_facility_\", area_name))\nhdb_facility_points <- hdb_facility %>% select(c(\"geometry\"))\narea_network <- get(paste0(\"network_\", area_name))\n\n# prepare lixel objects\nlixels <- lixelize_lines(area_network, 700, mindist = 350)\n\n# generate line centre points\nsamples <- lines_center(lixels)\n\n# performing NetKDE\n\ndensities <- nkde(area_network,\n                  events = hdb_facility_points,\n                  w = rep(1,nrow(hdb_facility_points)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 100, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n# add densities to samples and lixels\nsamples$density <- densities\nlixels$density <- densities\n\n# rescale densities to help with mapping\nsamples$density <- samples$density * 1000\nlixels$density <- lixels$density * 1000\n\n# save lixel as RDS with unique name\nlixel_name <- paste0(\"lixel_facility_\", area_name, \".rds\")\nsaveRDS(lixels, lixel_name)\n\n\nWe saved each lixel as “lixel_facility_area_name.rds” according to the facility and area names.\n\n\n3.4.2.2 Plotting out Lixels\nUsing the code below, we plotted out the road network density using the lixel generated earlier. We then plotted the HDB and facility points, colour-coding the points for easier visualisation.\n\n\nShow the code\ntmap_mode('view')\n\ntm_shape(shape_area_name) +\n\n  tm_polygons() +\n\ntm_shape(lixel_facility_area_name)+\n\n  tm_lines(col=\"density\", lwd=2)+\n\ntm_shape(hdb_facility_area_name)+\n\n  tm_dots(col = \"Point Type\", palette=c('blue', 'red')) +\n\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\n3.4.2.3 Observations\nOne thing we can observe is the stark difference between the ratio of the number of HDB points to the facility points. Since we have done our analysis based on the planning area, the number of HDB and facility points per planning area is rather small. Thus, the small change in number of facility points and their geometry location greatly affects the density results of our network KDE analysis.\nOur analysis showed that Punggol and Sembawang have the highest network density, followed by Choa Chu Kang, Bukit Batok, and Yishun. These regions are located in the north(-east) and west of Singapore. Conversely, the least dense network was observed in Tuas, which had only three HDB points. Overall, this information can be valuable for potential HDB buyers looking to make informed decisions based on their preferences and needs.\nHowever that being said, areas with little to no facility points gave us less insightful results as it is basically a network KDE analysis on the HDB points itself rather than that of both HDB and facility points. This is a rather recurring observation thus we decided to focus just on the HDB points without the facilities for the next test, K-function.\n\n\n\n3.4.3 K-Function Analysis\n\n3.4.3.1 Performing K-Function\nSimilarly, with reference to “R for Geospatial Data Science and Analytics”, we performed the K-function test for each planning area individually by changing the n for “area_names” as shown below.\n\n\nShow the code\n# set directory to save kfunc files\nsetwd(\"data/rds/kfunc\")\n\n# get data\narea_name <- area_names[n]\n\nhdb_facility <- get(paste0(\"point_\", area_name))\nhdb_facility_points <- hdb_facility %>% select(c(\"geometry\"))\narea_network <- get(paste0(\"network_\", area_name))\n\n# kfunc\nkfun <- kfunctions(area_network, \n                             hdb_facility_points,\n                             start = 0, \n                             end = 1000, \n                             step = 50, \n                             width = 50, \n                             nsim = 50, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05)\n\n# save kfunc as RDS with unique name\nkfunc_name <- paste0(\"kfunc_\", area_name, \".rds\")\nsaveRDS(kfun, kfunc_name)\n\n\n\n\n3.4.3.2 Plotting K-Function\nThe code below plots the K-function graph.\n\n\nShow the code\nkfunc_area_name$plotk\n\n\n\n\n\n\n3.4.4 Problems Faced\nAnother reason why we chose to only do the K-function analysis for HDB points rather than both HDB and facility points is because of the huge file size of each rds file saved. After completing the K-Function analysis for 12 facility points for 30 planning areas, we ended up with a total of 8.51GB for file size. We also contemplated if we wanted to save each K-function as a photo instead but loading over 360 photos together with other files we have into our shiny app would take too much time. Lastly, we also had the option to just load the K-function on the Shiny app itself but this can also take up a lot of time depending on the planning area. Thus,we ended up keeping the 30 K-function analysis that included only HDB points at 30 planning areas.\n\n\n3.4.5 Observations\nWe wanted to focus on the planning areas that we observed were the most and least dense as found out at the NetKDE analysis earlier. The blue line represents the empirical network K-function of the HDB points in the respective planning areas. The grey area represents the results of the 50 simulations in the interval 2.5% - 97.5% .\n\nPunggol:\nThe blue line is a lot higher than the grey area throughout the whole distance (0 to 1000m). Thus, we can interpret that the HDB points are more clustered than what is expected from a random distribution.\n\n\n\nFigure 6: NetKDE K-function of the HDB points in Punggol\n\n\nSembawang:\nThe blue line is a lot higher than the grey area from around 100m to 600m. Thus, we can interpret that the HDB points are more clustered at that distance than what is expected from a random distribution.\n\n\n\n\nFigure 7: NetKDE K-function of the HDB points in Sembawang\n\n\n\nTuas:\nThe blue line is at 0 across the whole distance and there is no grey area. This is in line with the network KDE analysis performed earlier. The HDB points are not clustered because there are only 3 points in such a huge area.\n\n\n\n\nFigure 8: NetKDE K-function of the HDB points in Tuas"
  },
  {
    "objectID": "user_guide.html",
    "href": "user_guide.html",
    "title": "User Guide for Our Shiny Application - The Right Place",
    "section": "",
    "text": "Welcome to the user guide for our application! When you first visit our application, you will see the homepage where we introduce the motivation behind our application, the authors and our acknowledgments.\n\nFrom there, you can find a navigation bar that allows you to easily switch between different pages. Please take note that all the content on the tab should be loaded completely before switching to another tab. This will prevent any delays or errors caused by incomplete loading of the page.\nThe navigation bar pages include:\n\nVisualization page: Here, you can observe the mapping of HDB flats and relevant amenities in Singapore.\n\n\nBy default, the page will show the mapping of an overview of the HDB locations in Singapore.\nThere are two dropdown lists for users to filter the visualisation.\n\nThe first dropdown allow users to filter visualisation of HDB flats and the relevant amenities.\nThe second dropdown allows users to filter the resale flat price ranges of the HDB flats they are interested in.\n\n\nSpatial Point Analysis: In this page, we offer several tools designed to help you analyze the spatial distribution of points in your dataset. Our tools include Local Colocation Quotient Analysis (CLQ), Kernel Density Estimation (KDE), F-Function, Ripley L-Function, and Network Constraint Analysis. To access these tools, simply click on the five tabs located at the top of the page. Each tool provides a unique perspective on the spatial patterns in your data, allowing you to gain valuable insights into the underlying processes driving your observations.\n\nLocal Colocation Quotient Analysis (CLQ)\n\nBy default, it will show the CLQ map for childcare centres.\nUsers can filter the CLQ maps based on the amenity type.\n\nKernel Density Estimation (KDE)\n\n\nBy default, it will show nothing until the user press the “Plot KDE graph” button.\nUsers can compute the KDE based on the amenity type, bandwidth type and kernel type.\nPlease take note that the KDE may take a while to compute and appear.\n\nF-Function\n\n\nBy default, it will show nothing until the user press the “Plot F-function graph” button.\nUsers can compute and plot the F-function based on an area of their choice.\nPlease take note that the F-function graph may take a while to compute and appear.\n\nRipley L-Function\n\n\nBy default, it will show nothing until the user press the “Plot L-function graph” button\nUsers can compute and plot the L-function based on an area of their choice.\nPlease take note that the L-function graph may take a while to compute and appear.\n\nNetwork Constraint Analysis\n\n\nBy default, it will show the lixel plot and K-function graph of area, Bedok and Amenity Type, Childcare.\nUsers select the the area of their choice and the amenity type.\n\n\nData Upload: Here, users can upload their data of an amenity in rds format. The uploaded data will then be available as an option under the visualisation page’s “What would you like to view?” drop-down list.\n\nBy clicking the “Browse button”, users can upload a rds file of any amenity they would like to visualise.\nBy clicking the “Preview Data”, users can see the top 10 rows of the rds file they uploaded.\n\nWe hope you find our application useful for your spatial data analysis needs. If you have any questions or feedback, please feel free to contact us though our github."
  },
  {
    "objectID": "project_proposal.html#application-design-overview",
    "href": "project_proposal.html#application-design-overview",
    "title": "G1T7 Project Research Paper",
    "section": "Application Design Overview",
    "text": "Application Design Overview\nOur Shiny application features a minimalist design, with a clean and modern look achieved through the use of the “minty” bootswatch theme. The layout is simple and intuitive, featuring a navigation bar that provides easy access to various features of the app. Users can explore HDB locations and amenities through a mapping visualization, analyze spatial point patterns with graphs, and even upload their own datasets for further analysis.\nTo provide a better understanding of our application, below are examples of our application screens. The user guide to our application can be found here.\n\nHomepage\nIn this page, it introduces the motivation of our application and the authors.\n\n\n\nFigure 10: Homepage\n\n\nVisualization page\nIn this page, we can observe the mapping of HDB flats and relevant amenities in Singapore.\n\n\n\nFigure 11: Mapping of HDB Flats and Relevant Amenities\n\n\nSpatial Point Analysis\nIn this page, we offer several tools designed to help you analyze the spatial distribution of points in your dataset. Our tools includes the Local Colocation Quotient Analysis (CLQ), Kernel Density Estimation (KDE), F-Function, Ripley L-Function, and Network Constraint Analysis. To access these tools, simply click on the five tabs located at the top of the page. Each tool provides a unique perspective on the spatial patterns in your data, allowing you to gain valuable insights into the underlying processes driving your observations.\n\n\n\nFigure 12: Spatial Point Analysis - CLQ Map\n\n\nData Upload\nIn this page, users can upload their data of an amenity in rds format and it will be shown as an option under the visualisation page, what would you like to view’s drop down list.\n\n\n\nFigure 13: Upload Dataset Page"
  },
  {
    "objectID": "research_paper.html",
    "href": "research_paper.html",
    "title": "G1T7 Project Research Paper",
    "section": "",
    "text": "This project aims to simplify the process of understanding amenities and facilities in the vicinity of desired HDB locations. The proposed analytical Shiny app utilizes advanced spatial point pattern analysis techniques to provide a customized view of amenities that match HDB buyers’ preferences. The project objectives include visualizing an overview of HDB flats in Singapore along with relevant amenity locations, measuring spatial association and heterogeneity, estimating HDB location intensity, assessing HDB location distribution non-randomness, and analyzing the spatial distribution of HDB flats over a street network. By empowering HDB buyers with a user-friendly application, the project seeks to assist them in making more informed decisions about their purchases and feeling more confident about their selected residence."
  },
  {
    "objectID": "research_paper.html#project-motivation",
    "href": "research_paper.html#project-motivation",
    "title": "G1T7 Project Research Paper",
    "section": "Project Motivation",
    "text": "Project Motivation\nMany prospective HDB buyers face challenges in visualising and understanding the amenities and facilities available in the vicinity of their desired location. They often resort to manual searches on platforms like Google, which can be time-consuming and frustrating. Moreover, current mapping tools do not offer a comprehensive and tailored view of amenities specific to HDBs.\nOur motivation is to simplify this process by providing a user-friendly analytical app that enables HDB buyers to view and understand the surrounding amenities easily. Our app utilizes advanced spatial point pattern analysis techniques to visualize the distribution and clustering of amenities relevant to HDB buyers. By providing a customized view of amenities that cater specifically to the needs of HDB buyers, we aim to empower HDB buyers to make more informed decisions about their purchases and feel more confident in their chosen residence."
  },
  {
    "objectID": "research_paper.html#project-objectives",
    "href": "research_paper.html#project-objectives",
    "title": "G1T7 Project Research Paper",
    "section": "Project Objectives",
    "text": "Project Objectives\nThe objective of this project is to develop an analytical application that empowers HDB buyers with the following capabilities:\n\nVisualise an overview of HDB flats in Singapore along with relevant amenity locations, such as shopping malls, bus stops, etc. that match their preferences.\nMeasure the level of spatial association and heterogeneity between HDB locations and surrounding amenities using Colocation Quotients (CLQs) analysis.\nUse Kernel Density Estimation (KDE) to estimate the intensity of HDB locations in different study areas of Singapore, such as Tampines and Bedok.\nAssess the non-randomness of HDB location distribution in selected study areas, such as Tampines and Bedok, using F-function and Ripley’s L-function.\nApply Network Constrained Spatial Point Patterns Analysis to analyze the spatial distribution of HDB flats over a street network.\n\nBy incorporating advanced spatial point pattern analysis techniques, we aim to simplify the process for HDB buyers by providing them with a user-friendly application that visualises amenities and facilities in their desired location. Our objective is to identify significant spatial patterns and trends that can assist buyers in making informed decisions about their HDB flat purchases and feel confident about their selected residence."
  },
  {
    "objectID": "research_paper.html#our-methodology-and-observations",
    "href": "research_paper.html#our-methodology-and-observations",
    "title": "G1T7 Project Research Paper",
    "section": "Our Methodology and Observations",
    "text": "Our Methodology and Observations\n\n1. Data Collection\nIn the initial stages of our project, we collected a range of datasets to analyze the spatial point patterns of HDB locations and amenities that we believe would be valuable to HDB buyers. These datasets comprise of various amenities such as MRT stations, bus stops, supermarkets, hawker centres, retail pharmacies, carparks, and more. We acquired some of these datasets online, while others were extracted using OneMap API.\n\n1.1 List of Datasets Used\nBelow is a table of the data sets that we will be using for our project.\n\n\n\nName\nDescription\nFile Format\n\n\n\n\nHDB Resale Flat Prices\nProvides HDB addresses, blocks and street name | Data.gov.sg\n.csv\n\n\nSchool Directory and Information\nProvides a list of primary schools in Singapore | Data.gov.sg\n.csv\n\n\nShopping Malls\nProvides a list of shopping mall and its geometry in Singapore | Web scrapped shopping mall data in 2019 by Valery Lim\n.csv\n\n\nURA Master Plan 2019 Subzone Boundary\nProvides region boundary data | Referenced/taken from Prof Kam\nshp\n\n\nBus Stops\nProvides a list of bus stops and its geometry in Singapore | Datamall LTA\n.shp\n\n\nTrain Station\nProvides a list of MRT/LTR exits and its geometry in Singapore | Datamall LTA\n.shp\n\n\nSupermarkets\nProvides a list of supermarkets in Singapore | Data.gov.sg\n.geojson\n\n\nChildcare Centres\nProvides a list of names, addresses and relevant information for childcare centres in Singapore | Extracted via onemapAPI API Docs | Registration\n.rds\n\n\nEldercare Centres\nProvides a list of names, addresses and relevant information for eldercare centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nKindergartens\nProvides a list of names, addresses and relevant information for kindergartens in Singapore | Extracted via onemapAPI\n.rds\n\n\nHawker Centres\nProvides a list of names, addresses and relevant information for hawker centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nHealthier Hawker Centres\nProvides a list of names, addresses and relevant information for healthier hawker centres in Singapore | Extracted via onemapAPI\n.rds\n\n\nNational Parks\nProvides a list of names, addresses and relevant information for national parks in Singapore | Extracted via onemapAPI\n.rds\n\n\nGyms\nProvides a list of names, addresses and relevant information for gyms in Singapore | Extracted via onemapAPI\n.rds\n\n\nRetail Pharmacies\nProvides a list of names, addresses and relevant information for retail pharmacies in Singapore | Extracted via onemapAPI\n.rds\n\n\nSingapore Police Force (SPF) Establishments\nProvides a list of names, addresses and relevant information for SPF establishments in Singapore | Extracted via onemapAPI\n.rds\n\n\nCarparks\nProvides a list of names, addresses and relevant information for carparks in Singapore | Extracted via onemapAPI\n.rds\n\n\nLibraries\nProvides a list of names, addresses and relevant information for libraries in Singapore | Extracted via onemapAPI | To be used as a sample dataset to test the upload dataset button in our Shiny application\n.rds\n\n\n\n\n\n1.2 Packages Used\nBelow is the list of packages we will be using for our project:\n\nshiny: to create the Shiny application\nshinyjs: to perform JavaScript operations on the Shiny application\nshinyWidget: to customise input widgets on the Shiny application\nleaflet: to create interactive map on the Shiny application\nbslib: to create a modern UI interface for the Shiny application using Bootstrap\nrvest: to parse HTML and XML files\njsonlite: to parse and generate JSON files\ndplyr: to simplify the data manipulation tasks such as filtering rows, selecting columns, grouping and summarizing data, and joining multiple datasets\nsf: to import and handle geospatial data\nsp: to import and handle spatial point and lines data frames\nrdgal: to import geospatial data and store them as sp objects\nreadxl: to read and import excel files\ntidyverse: to handle data wrangling (tidyr, dplyr, ggplot2, tibble)\nmaptools: to manipulate geographic data\ndata.table: to speed up the modification of data\nhttr: to run API requests (for getting data from onemapsg)\ntmap: to plot out choropleth maps\nggplot2: to create elegant data visualisation for mapping\nggpubr: to customise ggplots for better visualisation\nggthemes: to add preset themes, geoms and scales for ggplot2\nplotly: to create interactive web graphics from ggplot2 graphs\nfunModeling: to plot EDA with better visualisation\nraster: to convert grid output to raster layer for visualisation (Kernel Density Estimation)\nspatstat: to convert spatial objects to ppp format (Spatial Point Pattern plot)\nolsrr: to build OLS regression models\nsfdep: to find contiguous neighbours, calculate weights and perform LISAs (Local Indicator of Spatial Association)\nspNetwork: to perform spatial analysis on networks. Includes network kernel density estimations as well as K function estimations for point pattern analysis on the network.\nvctrs: provides a framework for working with vectors and arrays in a consistent and efficient way.\n\n\n\n\n\n2. Initial Data Pre-processing\nTo optimize the loading speed of the application, we carefully considered the size of the HDB location dataset (i.e HDB Resale Flat Prices), as its large volume may lead to slow loading times. Consequently, we extracted the HDB locations of 5-room flat types over a one-year period from January 2022 to December 2022. We then preprocessed all the datasets by extracting relevant columns, removing missing values, and ensuring that the geometries are valid and have the correct coordinate reference system (CRS) information, specifically EPSG:3414. Afterwards, we carried out our spatial point analysis mentioned under our objectives.\n\n\n3. Spatial Point Analysis\n\n3.1 Measure the level of spatial association and heterogeneity between HDB locations and surrounding amenities using Colocation Quotients (CLQs) analysis\n\n3.1.1 Data Preparation\nBefore we can perform CLQ, we need to extract the datasets to contain the type of amenity with its respective geometry without the address names of the amenities. For example, the code chunk below extracts out the geometry of the HDB locations and names it as type “HDB”.\n\n\nShow the code\nhdb_a <- resale_sf %>%\n  select(geometry) %>% \n  mutate(geometry= geometry,\n         type = \"HDB\") \n\n\n\n\n3.1.2 Computing Colocation Quotients (CLQs)\nTo streamline the process of computing Colocation Quotients (CLQs) for multiple amenities, a function called cql_func(), which takes in the HDB extracted dataset, a sf dataframe of the amenity that has yet to be extracted and the name of the amenity, was created to extract the relevant datasets, perform the necessary computations, and save the output to an rds file. This enables easy import and visualization of the results in the Shiny app. Below are the steps of the function created:\n\nCreate the extracted dataset for the amenity\nCombine HDB and Amenity dataset into a single dataframe\nPrepare nearest neighbours list by using st_knn() of sfdep package to determine the k (i.e. 6) nearest neighbours for given point geometry.\nCompute kernel weights by using st_kernel_weights() of sfdep package to derive a weights list by using a kernel function.\nPrepare a vector list to reference the points data. For example, A list will be the HDB location points and B list will be the location points of another amenity such as Hawker Centres\nCompute the LCLQ values for each point event of HDB points using local_colocation() \nJoin the output of local_colocation() to the previous HDB points and amenities data.frame and write it into an rds file\n\nBelow is the code chunk for the function created.\n\n\nShow the code\ncql_func <- function(df1,df2,nameB){\n  df2_b <- df2 %>%\n  select(geometry) %>% \n  mutate(geometry= geometry,\n         type = nameB) \n  combined <- rbindlist(list(df1, df2_b),use.names=FALSE)\n  combined_sf <- st_as_sf(combined) %>%\n  st_transform(crs = 3414)\n  \n  nb <- include_self(\n  st_knn(st_geometry(combined_sf), 6))\n  \n  wt <- st_kernel_weights(nb, \n                        combined_sf, \n                        \"gaussian\", \n                        adaptive = TRUE)\n  \n  hdb_locations <- combined_sf %>%\n  filter(type == \"HDB\")\n  A <- hdb_locations$type\n  \n  df2_locations <- combined_sf %>%\n  filter(type == nameB)\n  \n  B <- df2_locations$type\n  \n  LCLQ <- local_colocation(A, B, wt, nb, 99) # 100 simulations\n  LCLQ_stores <- cbind(combined_sf, LCLQ)\n  \n  write_rds(LCLQ_stores, paste0(\"data/rds/clq/hdb_\",nameB,\".rds\"))\n  }\n\n\n\nWhen visualizing the results in the Shiny app, I filtered the data to display only the p-values that are less than 0.05. This allows users to easily identify the significant associations between HDB locations and amenities. The code chunk can be referred to below.\n\n\nShow the code\n clq_map <- tm_shape(mpsz_sf) +\n      tm_polygons() +\n      tm_shape(result, subset = result$amenity_pvalue < 0.05) + #filter out p_value less than 0.05 \n      tm_dots(col = amenity_name,\n              size = 0.01,\n              border.col = \"black\",\n              border.lwd = 0.5) +\n      tm_view(set.zoom.limits = c(11, 14))\n    \n    output$clq_outputmap <- renderLeaflet(tmap_leaflet(clq_map))\n\n\n\n\n3.1.3 Observations\nBy looking at the LCLQ map for HDB locations and Hawker centres below, we can observe that there are not many missing values and the local colocation quotient of 0.992. This informs users that the points are isolated but significant as the local colocation quotient is less than 1 with a p-value less than 0.05. The information provided about the LCLQ map for HDB locations and Hawker centres can be useful for HDB buyers in several ways. Firstly, it suggests that there is a high likelihood that HDB flats in the area are located near hawker centres. This can be important for buyers who prioritise having access to food options and dining facilities nearby. Secondly, the fact that the local colocation quotient is close to 1 and the p-value is less than 0.05 indicates that the spatial relationship between HDB locations and hawker centres is not likely to have occurred by chance. This could be interpreted as evidence of intentional planning or zoning by the authorities, which may indicate the area has been designed to meet the needs of residents.\n\n\n\n\nFigure 1: CLQ Map of Hawker Centres in relation to HDB\n\n\n\n\n\n3.2 Use Kernel Density Estimation (KDE) to estimate the intensity of HDB locations in different study areas of Singapore, such as Tampines and Bedok\n\n3.2.1 Data Preparation\nBefore we can perform KDE, we need to first ensure that our sf dataframes for all the amenities type are converted into sp’s Spatial* class, then converted to the Spatial* class into generic sp format, then converted into spatstat’s ppp format and finally converted into an owin object. The code chunk below is an example of us converting the subzone and HDB dataset into an owin object.\n\n\nShow the code\nmpsz <- as_Spatial(mpsz_sf)\nmpsz_sp <- as(mpsz, \"SpatialPolygons\")\nmpsz_owin <- as(mpsz_sp, \"owin\")\n\nresale <- as_Spatial(resale_sf)\nresale_sp <- as(resale, \"SpatialPoints\")\nresale_ppp <- as(resale_sp, \"ppp\")\nresaleSG_ppp = resale_ppp[mpsz_owin]\n\n\nBut before we converted it into an owin, we also made sure to check for any duplicated values and dealt with it by implementing the jittering approach, which adds a small perturbation to the duplicate points so that they do not occupy the exact same space.\n\n\nShow the code\nany(duplicated(resale_ppp))\nmultiplicity(resale_ppp)\nsum(multiplicity(resale_ppp) > 1)\nresale_ppp_jit <- rjitter(resale_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\nFrom there onwards, we rescaled the unit of measurement from metre to kilometre using the rescale() function and saved the file into an rds file so we can visualise it in our application. The code chunk below shows an example of us rescaling the spf, mrt, gyms and HDB datasets and writing it into an rds file.\n\n\nShow the code\nspfSG_ppp.km <- rescale(spfSG_ppp, 1000, \"km\")\nmrtSG_ppp.km <- rescale(mrtSG_ppp, 1000, \"km\")\ngymsSG_ppp.km <- rescale(gymsSG_ppp, 1000, \"km\")\nresaleSG_ppp.km <- rescale(resaleSG_ppp, 1000, \"km\")\nsaveRDS(spfSG_ppp.km, \"data/rds/kde/spfSG_ppp.km.rds\")\nsaveRDS(mrtSG_ppp.km, \"data/rds/kde/mrtSG_ppp.km.rds\")\nsaveRDS(gymsSG_ppp.km, \"data/rds/kde/gymsSG_ppp.km.rds\")\nsaveRDS(resaleSG_ppp.km, \"data/rds/kde/hdbSG_ppp.km.rds\")\n\n\n\n\n3.2.2 Computing Kernel Density Estimation (KDE)\nThe following steps are taken to compute the KDE:\n\nRead the rds file of the amenity chosen by user\nCompute the KDE based on the bandwidth and kernel type selected by the user\nRender the KDE plot in the application\n\nThe code chunk below shows how we can compute and plot the KDE.\n\n\nShow the code\nindex_kde <- match(input$kde_amenity, names(kde_files))\n    kde_name <- kde_files[index_kde]\n    kde_rds <- readRDS(paste0(\"data/rds/kde/\",kde_name,\".rds\"))\n    \n    \n# Calculate kernel density estimate\nkde_result <- density(kde_rds, \n                          sigma = get(input$kde_bw), \n                          edge = TRUE, \n                          kernel = input$kde_kernel)\n    \n# Render plot\noutput$kde_plot <- renderPlot({\n      plot(kde_result) \n  })\n\n\nBased on the KDE plot, users will be able to identify high or low concentrations of the amenity type. The lighter the color of the cluster, the higher concentration of the amenity type.\n\n\n3.2.3 Observations\nBased on the graph below, we used the spatstat package’s density() function and the Gaussian kernel with the bw.diggle() automatic bandwidth selection method to generate a kernel density estimation (KDE) plot for HDB flats. Specifically, we observed a higher concentration of HDB flats in the northeast side of Singapore compared to other regions. This information may be useful for potential buyers who prefer to live in areas with fewer HDB flats to avoid the northeast side of Singapore and focus their property search on other regions.\n\n\n\nFigure 2: KDE(bw.diggle(), Gaussian kernel) Map of HDB flats\n\n\nIn our application, we also allow users to select the bandwidth type and kernel used to compute the KDE. This allows them to customise the type of KDE graph they would like to view.\n\n\n\nFigure 3: KDE(bw.scott(), Quartic kernel) Map of HDB flats\n\n\n\n\n\n3.3 Assess the non-randomness of HDB location distribution in selected study areas, such as Tampines and Bedok, using F-function and Ripley’s L-function.\n\n3.3.1 Data Preparation\nFor the F-function and Ripley’s L-function, we are able to reuse the rds files for the HDB locations and subzone dataset that we have prepared for the KDE. The only difference is that we will be extracting HDB locations that are within a specific area such as Bedok, Tampines, etc. The code chunk below is an example of extracting HDB locations that are within Bedok.\n\n\nShow the code\nbedok = mpsz[mpsz@data$PLN_AREA_N == input$ffunc,]\n    bedok_sp = as(bedok, \"SpatialPolygons\")\n    bedok_owin = as(bedok_sp, \"owin\")\n    resale_bedok_ppp = resale_ppp_jit[bedok_owin]\n\n\n\n\n3.3.2 Computing F-function and Ripley’s L-function\nNext, we can compute an estimation of the F-function and L-function, perform a complete spatial randomness(CSR) test, where the null hypothesis is that the distribution of HDB locations in the user’s chosen area is random, and the alternative hypothesis is that the distribution of HDB locations in the user’s chosen area is non-random and plot the F-function and L-function graph to visualize the results of the analysis.\nThe following code chunk demonstrates how we computed the estimation of the F-function and L-function. Since the F-function and L-function is based on a simulation, we set the random seed to 123 to ensure the reproducibility of the analysis. Additionally, we perform a test at a significance level of 0.05, so the value of alpha will be 0.05 and nsim(the number of simulated point patterns to be generated when computing the envelope.) will be 39.\n\n\nShow the code\nset.seed(123)\nF_bedok.csr <- envelope(resale_bedok_ppp, Fest, nsim = 39)\nL_bedok.csr <- envelope(resale_bedok_ppp, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\n\nWith regards to the L-function, it is an interactive map where we referenced the code from our senior, Denise Adele Chua.\nBased on the resulting plots, users will be able to observe the F-function or L-function (a solid black line) and the envelope (shaded area), which represents the range of values that would be expected under CSR test. If the observed F-function or L-function lies outside the envelope, it suggests that the data depart significantly from the CSR test, indicating either clustering or regularity in the point pattern. If the observed F-function or L-function lies inside the envelope, it suggests that the data are consistent with CSR, meaning that the pattern of the point locations is random, and no significant spatial clustering or regularity is present in the data.\n\n\n3.3.3 Observations\nBelow is an example of the F-function graph of the Bedok area. We can observed that the F-function and L-function lies outside the envelope, this suggests that the HDB data depart significantly from CSR test.\n\n\n\nFigure 4: F-function graph for HDB flats in Bedok\n\n\n\n\n\nFigure 5: L-function graph for HDB flats in Bedok\n\n\n\n\n\n3.4 Apply Network Constrained Spatial Point Patterns Analysis to analyse the spatial distribution of HDB flats over a street network.\nLastly, we conducted a network constrained spatial point pattern analysis on our datasets, using the package spNetwork. We performed a network KDE as well as the K-Function analysis.\n\n3.4.1 Data Preparation\nFirst, we split all our data sets by the planning area (“PLN_AREA_N”). This included the HDB points, facilities points, mpsz as well as the road network. There are only 30 planning areas that contain HDB points, we will only be using these 30 areas.\n\n3.4.1.1 Splitting mpsz\nWe split the multipolygon shapefile mpsz_sf into smaller subsets by the PLN_AREA_N (planning area name). We then saved each subset named “shape_PLN_AREA_N.rds” according to the planning area.\n\n\nShow the code\n# splitting mpsz by PLN_AREA_N\nfor (x in unique(mpsz_sf$PLN_AREA_N)) {\n  \n  # filter the polygons data frame to get only the polygons for the current area\n  area_polygons <- mpsz_sf %>%\n    filter(PLN_AREA_N == x) %>%\n    st_as_sf() # convert to sf object if necessary\n  \n  # replace spaces with underscores in the area name\n  area_name_underscore <- gsub(\" \", \"_\", x)\n  \n  # construct the name of the output file\n  output_filename <- paste0(\"shape_\", area_name_underscore, \".rds\")\n  \n  # save the area polygons as an RDS file\n  write_rds(area_polygons, file = output_filename)\n}\n\n\n\n\n3.4.1.2 Splitting Road Networks\nNext, we split the road networks based on the intersection of the road networks and the planning areas. We used the st_intersection() function and then saved each intersection as an rds file named “network_PLN_AREA_N.rds” according to the planning area.\n\n\nShow the code\ngrouped_polygons <- mpsz_sf %>%\n  group_by(PLN_AREA_N) %>%\n  summarize(geometry = st_combine(geometry)) %>%\n  st_as_sf()\nintersected <- st_intersection(road_network_lines, grouped_polygons)\nfor (PLN_AREA_N in unique(intersected$PLN_AREA_N)) {\n  filtered <- intersected[intersected$PLN_AREA_N == PLN_AREA_N,]\n  write_rds(filtered, paste0(\"data/geospatial/network/network_\", PLN_AREA_N, \".rds\"))\n}\n\n\n\n\n3.4.1.3 Splitting and Combining HDB and Facility Points\nLooping through the planning areas, we first find the HDB spatial points and facility spatial points respectively in each planning area of the mpsz multipolygon using the st_intersection() function. We selected the geometry column and added a column “Point Type” labelling each row as either “HDB” or “Facility Name”. We combined the HDB and facility spatial points into a list then wrote it as a rds file named “hdb_facility_PLN_AREA_N.rds” for easy future importation.\n\n\nShow the code\n# initialise an empty list to store the output dataframes\noutput_dfs <- list()\n\n# loop over the unique area_names for each multipolygon\nfor (x in unique(mpsz_sf$PLN_AREA_N)) {\n  # subset the multipolygon dataframe by area_name\n  area_multipolygons <- mpsz_sf[mpsz_sf$PLN_AREA_N == x,]\n  # perform a spatial join between the points1 and area_multipolygons dataframes\n  points1_in_polygons <- st_intersection(hdb_points, area_multipolygons)\n  points1_in_polygons <- points1_in_polygons %>%\n    select(c(\"geometry\")) %>%\n    mutate(`Point Type` = \"HDB\") %>%\n    relocate(`Point Type`, .before = 1)\n  \n  # perform a spatial join between the facility points and area_multipolygons dataframes\n  points2_in_polygons <- st_intersection(facility_sf, area_multipolygons)\n  points2_in_polygons <- points2_in_polygons %>%\n    select(c(\"geometry\")) %>%\n    mutate(`Point Type` = \"Facility Name\") %>%\n    relocate(`Point Type`, .before = 1)\n  # create a unique name for the output dataframe\n  output_name <- paste0(\"hdb_facility_\", gsub(\" \", \"_\", x))\n  # store the output data frames in the output_dfs list\n  output_dfs[[output_name]] <- bind_rows(points1_in_polygons, points2_in_polygons)\n}\n\n# write the output data frames to RDS files\n\nfor (i in seq_along(output_dfs)) {\n  filename <- gsub(\" \", \"_\", names(output_dfs)[i]) # replace spaces with underscore\n  write_rds(output_dfs[[i]], paste0(filename, \".rds\")) # save each data frame as an RDS file\n}\n\n\n\n\n\n3.4.2 NetKDE Analysis\nIn earlier steps while splitting the HDB spatial points by the planning area, we realised that out of the 55 planning areas, only 30 planning areas include HDB spatial points. The area names are as shown below:\n\n\nShow the code\n# area names\narea_names <- c(\"MARINE PARADE\", \"BUKIT MERAH\", \"QUEENSTOWN\",\"OUTRAM\", \"ROCHOR\", \"KALLANG\",\"TANGLIN\", \"CLEMENTI\", \"BEDOK\", \"JURONG EAST\", \"GEYLANG\", \"BUKIT TIMAH\",\"NOVENA\", \"TOA PAYOH\",\"TUAS\", \"JURONG WEST\",\"SERANGOON\", \"BISHAN\",\"TAMPINES\", \"BUKIT BATOK\",\"HOUGANG\", \"ANG MO KIO\",\"PASIR RIS\", \"BUKIT PANJANG\", \"YISHUN\", \"PUNGGOL\",\"CHOA CHU KANG\", \"SENGKANG\",\"SEMBAWANG\", \"WOODLANDS\")\narea_names <- gsub(\" \", \"_\", area_names)\n\n\n\n3.4.2.1 Performing the Analysis\nSince our main focus is on HDB points, we will only cover these 30 planning areas moving forward for the analysis. We took reference to “R for Geospatial Data Science and Analytics” to perform the NetKDE analysis below.\nSince there are a lot of points and networks to run through the analysis, running it on loop will result in the immediate crashing of RStudio. Thus we had to go through each planning area and facility manually by editing the facility and area name ‘n’ below.\n\n\nShow the code\nsetwd(\"data/rds/lixel/facility\")\n\n# get data\narea_name <- area_names[n]\n\nhdb_facility <- get(paste0(\"hdb_facility_\", area_name))\nhdb_facility_points <- hdb_facility %>% select(c(\"geometry\"))\narea_network <- get(paste0(\"network_\", area_name))\n\n# prepare lixel objects\nlixels <- lixelize_lines(area_network, 700, mindist = 350)\n\n# generate line centre points\nsamples <- lines_center(lixels)\n\n# performing NetKDE\n\ndensities <- nkde(area_network,\n                  events = hdb_facility_points,\n                  w = rep(1,nrow(hdb_facility_points)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 100, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n# add densities to samples and lixels\nsamples$density <- densities\nlixels$density <- densities\n\n# rescale densities to help with mapping\nsamples$density <- samples$density * 1000\nlixels$density <- lixels$density * 1000\n\n# save lixel as RDS with unique name\nlixel_name <- paste0(\"lixel_facility_\", area_name, \".rds\")\nsaveRDS(lixels, lixel_name)\n\n\nWe saved each lixel as “lixel_facility_area_name.rds” according to the facility and area names.\n\n\n3.4.2.2 Plotting out Lixels\nUsing the code below, we plotted out the road network density using the lixel generated earlier. We then plotted the HDB and facility points, colour-coding the points for easier visualisation.\n\n\nShow the code\ntmap_mode('view')\n\ntm_shape(shape_area_name) +\n\n  tm_polygons() +\n\ntm_shape(lixel_facility_area_name)+\n\n  tm_lines(col=\"density\", lwd=2)+\n\ntm_shape(hdb_facility_area_name)+\n\n  tm_dots(col = \"Point Type\", palette=c('blue', 'red')) +\n\ntm_view(set.zoom.limits = c(14,16))\n\n\n\n\n3.4.2.3 Observations\nOne thing we can observe is the stark difference between the ratio of the number of HDB points to the facility points. Since we have done our analysis based on the planning area, the number of HDB and facility points per planning area is rather small. Thus, the small change in number of facility points and their geometry location greatly affects the density results of our network KDE analysis.\nOur analysis showed that Punggol and Sembawang have the highest network density, followed by Choa Chu Kang, Bukit Batok, and Yishun. These regions are located in the north(-east) and west of Singapore. Conversely, the least dense network was observed in Tuas, which had only three HDB points. Overall, this information can be valuable for potential HDB buyers looking to make informed decisions based on their preferences and needs.\nHowever that being said, areas with little to no facility points gave us less insightful results as it is basically a network KDE analysis on the HDB points itself rather than that of both HDB and facility points. This is a rather recurring observation thus we decided to focus just on the HDB points without the facilities for the next test, K-function.\n\n\n\n3.4.3 K-Function Analysis\n\n3.4.3.1 Performing K-Function\nSimilarly, with reference to “R for Geospatial Data Science and Analytics”, we performed the K-function test for each planning area individually by changing the n for “area_names” as shown below.\n\n\nShow the code\n# set directory to save kfunc files\nsetwd(\"data/rds/kfunc\")\n\n# get data\narea_name <- area_names[n]\n\nhdb_facility <- get(paste0(\"point_\", area_name))\nhdb_facility_points <- hdb_facility %>% select(c(\"geometry\"))\narea_network <- get(paste0(\"network_\", area_name))\n\n# kfunc\nkfun <- kfunctions(area_network, \n                             hdb_facility_points,\n                             start = 0, \n                             end = 1000, \n                             step = 50, \n                             width = 50, \n                             nsim = 50, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05)\n\n# save kfunc as RDS with unique name\nkfunc_name <- paste0(\"kfunc_\", area_name, \".rds\")\nsaveRDS(kfun, kfunc_name)\n\n\n\n\n3.4.3.2 Plotting K-Function\nThe code below plots the K-function graph.\n\n\nShow the code\nkfunc_area_name$plotk\n\n\n\n\n\n\n\n\n3.4.4 Problems Faced\nAnother reason why we chose to only do the K-function analysis for HDB points rather than both HDB and facility points is because of the huge file size of each rds file saved. After completing the K-Function analysis for 12 facility points for 30 planning areas, we ended up with a total of 8.51GB for file size. We also contemplated if we wanted to save each K-function as a photo instead but loading over 360 photos together with other files we have into our shiny app would take too much time. Lastly, we also had the option to just load the K-function on the Shiny app itself but this can also take up a lot of time depending on the planning area. After deploying the application, we realised that pushing even just the 30 rds files of around 1 GB was too much for the Shiny application to run. Thus,we ended up keeping the 30 K-function analysis pictures that included only HDB points at 30 planning areas.\n\n\n3.4.5 Observations\nWe wanted to focus on the planning areas that we observed were the most and least dense as found out at the NetKDE analysis earlier. The blue line represents the empirical network K-function of the HDB points in the respective planning areas. The grey area represents the results of the 50 simulations in the interval 2.5% - 97.5% .\n\nPunggol:\nThe blue line is a lot higher than the grey area throughout the whole distance (0 to 1000m). Thus, we can interpret that the HDB points are more clustered than what is expected from a random distribution.\n\n\n\nFigure 6: NetKDE K-function of the HDB points in Punggol\n\n\nSembawang:\nThe blue line is a lot higher than the grey area from around 100m to 600m. Thus, we can interpret that the HDB points are more clustered at that distance than what is expected from a random distribution.\n\n\n\n\nFigure 7: NetKDE K-function of the HDB points in Sembawang\n\n\n\nTuas:\nThe blue line is at 0 across the whole distance and there is no grey area. This is in line with the network KDE analysis performed earlier. The HDB points are not clustered because there are only 3 points in such a huge area.\n\n\n\n\nFigure 8: NetKDE K-function of the HDB points in Tuas\n\n\n\n\n\n3.5 General Problems Faced\nWhen we deployed our Shiny app, we realized that it was using too much memory, which caused performance issues. As a result, we had to find a solution that would allow us to display the images in the app without actually computing them in real-time. To achieve this, we had to generate the images beforehand and then render them in the app.\nFor example, in the case of the CLQ map, we had to modify our code to generate the plot as an image file and save it to disk. Then, we used the renderImage() function in Shiny to display the image in the app. This approach helped us reduce the memory usage of the app and improve its performance.\nHere’s an example of the modified code:\n\n\nShow the code\noutput$clq_outputmap <- renderImage({\n    filename <- normalizePath(file.path('./images',\n                                          paste(input$clq_amenities, '.PNG', sep='')))\n      \n    # Return a list containing the filename and alt text\n  list(src = filename, \n           width = \"80%\", \n           height = \"auto\")\n      }, deleteFile = FALSE\n    )"
  },
  {
    "objectID": "research_paper.html#literature-review",
    "href": "research_paper.html#literature-review",
    "title": "G1T7 Project Research Paper",
    "section": "Literature Review",
    "text": "Literature Review\n\n1. MEPHAS: an interactive graphical user interface for medical and pharmaceutical statistical analysis with R and Shiny (Zhou et al, 2020)\nThis article highlights that the MEPHAS tool has been designed to support various statistical analyses, including probability, hypothesis testing, regression modeling, and dimensional analysis. Each design interface contains multiple tabs for different analysis methods, an input panel on the left, and an output panel on the right. MEPHAS also allows for data input, parameter configuration, and result output.\n\nHow does it link to our project?\nWe plan to follow and utilise a similar design of the MEPHAS tool, making it a useful template for structuring our HDB app. We will be including navigation bar/tab panels to allow users to switch between the different spatial point analysis tools and implementing a side panel to allow users customize the visualizations and a main panel to generate the graph outputs.\n\n\n\n2. Analysing the global and local spatial associations of medical resources across Wuhan city using POI data (Chen, Q et al., 2023)\nThis article highlights the issue of imbalance in the supply and demand of medical resources in provincial capitals of China and emphasises the need to understand the spatial patterns of medical resources to ensure fair and optimal allocation of limited resources. This article utilises the Localised Colocation Quotient (LCLQ) analysis, which is a technique that measures directional spatial associations and heterogeneity between categorical point data. By employing this method and utilising point of interest (POI) data, the study presents a unique analysis of the spatial patterns and directional spatial associations between six medical resources in Wuhan city.\n\nHow does it link to our project?\nFor our project, we need to first define the two types of features of interest which will be HDB locations and the presence of certain amenities. Some examples are schools, carparks, shopping centres and MRT stations. By comparing the observed and expected frequencies of co-occurrence using the LCLQ, we can determine whether HDB locations and surrounding amenities are spatially associated in a non-random way. This can provide insights into the degree to which the availability of amenities in an area affects HDB locations, and vice versa.\n\n\n\n3. Measuring Spatial Patterns of Health Care Facilities and Their Relationships with Hypertension Inpatients in a Network-Constrained Urban System (Wang and Ke, 2019)\nThis study applied point pattern analysis (PPA) to investigate the spatial distribution of health care facilities in Shenzhen, China. Traditional PPA methods assume that spatial events are randomly located on a plane, but this is not appropriate for network-constrained events, such as those that occur on urban road networks. Therefore, the study used network-based analysis methods, such as network Kernel density estimation and network K-function.\n\nHow does it link to our project?\nWe plan to utilise the netKDE techniques employed in the article to estimate the density of events, such as the density of HDB locations, within a network-constrained environment. The purpose of using netKDE for HDB locations is to identify areas of high or low concentration of HDB units based on their proximity to the road network. With this analysis, HDB buyers can understand the density of HDB units in different regions of Singapore by identifying regions with a high concentration of HDB units, which may be indicative of a high demand for housing in those areas."
  },
  {
    "objectID": "research_paper.html#application-system-architecture",
    "href": "research_paper.html#application-system-architecture",
    "title": "G1T7 Project Research Paper",
    "section": "Application System Architecture",
    "text": "Application System Architecture\n\n\n\nFigure 9: Application Architecture Design"
  },
  {
    "objectID": "research_paper.html#application-design-overview",
    "href": "research_paper.html#application-design-overview",
    "title": "G1T7 Project Research Paper",
    "section": "Application Design Overview",
    "text": "Application Design Overview\nOur Shiny application features a minimalist design, with a clean and modern look achieved through the use of the “minty” bootswatch theme. The layout is simple and intuitive, featuring a navigation bar that provides easy access to various features of the app. Users can explore HDB locations and amenities through a mapping visualization, analyze spatial point patterns with graphs, and even upload their own datasets for further analysis.\nTo provide a better understanding of our application, below are examples of our application screens. The user guide to our application can be found here.\n\nHomepage\nIn this page, it introduces the motivation of our application and the authors.\n\n\n\nFigure 10: Homepage\n\n\nVisualization page\nIn this page, we can observe the mapping of HDB flats and relevant amenities in Singapore.\n\n\n\nFigure 11: Mapping of HDB Flats and Relevant Amenities\n\n\nSpatial Point Analysis\nIn this page, we offer several tools designed to help you analyze the spatial distribution of points in your dataset. Our tools includes the Local Colocation Quotient Analysis (CLQ), Kernel Density Estimation (KDE), F-Function, Ripley L-Function, and Network Constraint Analysis. To access these tools, simply click on the five tabs located at the top of the page. Each tool provides a unique perspective on the spatial patterns in your data, allowing you to gain valuable insights into the underlying processes driving your observations.\n\n\n\nFigure 12: Spatial Point Analysis - CLQ Map\n\n\nData Upload\nIn this page, users can upload their data of an amenity in rds format and it will be shown as an option under the visualisation page, what would you like to view’s drop down list.\n\n\n\nFigure 13: Upload Dataset Page"
  },
  {
    "objectID": "research_paper.html#future-work",
    "href": "research_paper.html#future-work",
    "title": "G1T7 Project Research Paper",
    "section": "Future Work",
    "text": "Future Work\nTo enhance the usefulness of our app, we can integrate hot and cold spot analysis of amenities to identify areas with a high concentration of amenities, known as hot spots, and areas with a low concentration of amenities, known as cold spots. While our current application uses KDE to estimate the density of amenities across an area, hot and cold spot analysis may offer a more intuitive approach to identifying areas with significant clusters of high or low concentrations. By examining the spatial distribution of amenities, we can pinpoint specific areas and provide more targeted information to HDB buyers. This approach may allow us to deliver more actionable insights and help buyers make more informed decisions about their property search.\nCurrently, our application’s dataset upload function can only upload rds files and is only applicable for visualizations of the map of amenities and HDB locations. To improve the functionality of the app, we can expand the dataset upload function to include other features such as plotting the CLQ maps, KDE, F-function, L-function, and network constraint analysisgraphs. This expansion will allow users to analyze them for deeper insights. Moreover, we can provide additional data pre-processing features to enhance the quality of the dataset uploaded by the user. This ensures that the data is effectively utilized to gain valuable insights and make informed decisions. By offering these advanced features, our application can provide more accurate and relevant information to users and help them make more informed decisions about their property search.\nWe can also enhance our app by integrating it with APIs (Application Programming Interfaces) that provide real-time location data for amenities such as supermarkets, shopping malls, and hawkers in Singapore. By doing so, our app can obtain the most up-to-date information on the availability and locations of these amenities. This integration will ensure that the information provided by our application is accurate and reliable, which is essential for making informed decisions."
  },
  {
    "objectID": "research_paper.html#timeline",
    "href": "research_paper.html#timeline",
    "title": "G1T7 Project Research Paper",
    "section": "Timeline",
    "text": "Timeline"
  },
  {
    "objectID": "research_paper.html#references",
    "href": "research_paper.html#references",
    "title": "G1T7 Project Research Paper",
    "section": "References",
    "text": "References\nSpatial First Point Analysis (link)\nSpatial Second Point Analysis (link)\nCLQ Interpretation (link)\nNetwork Constrained Spatial Point Patterns Analysis (link)\nSenior’s Interactive L-function code (link)\nShiny Documentation (link)\nLiterature review No.1 (link)\nLiterature review No.2 (link)\nLiterature review No.3 (link)"
  },
  {
    "objectID": "code_draft/mapping_clq.html",
    "href": "code_draft/mapping_clq.html",
    "title": "Mapping and LCLQ",
    "section": "",
    "text": "pacman::p_load(readxl, sf, tidyverse, tmap, sfdep,  ggpubr, plotly, sfdep, data.table)"
  },
  {
    "objectID": "code_draft/mapping_clq.html#import-data",
    "href": "code_draft/mapping_clq.html#import-data",
    "title": "Mapping and LCLQ",
    "section": "Import Data",
    "text": "Import Data\n\n#import data\nresale_sf<-readRDS(\"data/rds/resale_sf.rds\")\n\nmpsz_sf<-readRDS(\"data/rds/mpsz_sf.rds\")\n\nchildcare_sf <- readRDS(\"data/rds/childcare_sf.rds\")\neldercare_sf<- readRDS(\"data/rds/eldercare_sf.rds\")\nkindergartens_sf <- readRDS(\"data/rds/kindergartens_sf.rds\")\nhawkercentre_new_sf <- readRDS(\"data/rds/hawkercentre_new_sf.rds\")\nhawkercentre_healthy_sf<-readRDS(\"data/rds/hawkercentre_healthy_sf.rds\")\nnationalparks_sf<-readRDS(\"data/rds/nationalparks_sf.rds\")\ngyms_sf <-readRDS(\"data/rds/gyms_sf.rds\")\npharmacy_sf<-readRDS(\"data/rds/pharmacy_sf.rds\")\nspf_sf<-readRDS(\"data/rds/spf_sf.rds\")\nHDB_carpark_sf<-readRDS(\"data/rds/HDB_carpark_sf.rds\")\n\nsupermarket_sf<-readRDS(\"data/rds/supermarket_sf.rds\")\nbus_stop_sf<-readRDS(\"data/rds/bus_stop_sf.rds\")\nmrt_sf<-readRDS(\"data/rds/mrt_sf.rds\")\nprimary_school_sf<-readRDS(\"data/rds/primary_school_sf.rds\")\ntop10_primary_school_sf<-readRDS(\"data/rds/top10_primary_school_sf.rds\")\nshopping_mall_sf<-readRDS(\"data/rds/shopping_mall_sf.rds\")\n\n# combine street name and blk for HDB full address\nresale_sf$full_address <- paste(\"BLK\", resale_sf$block, resale_sf$street_name)"
  },
  {
    "objectID": "code_draft/mapping_clq.html#mapping",
    "href": "code_draft/mapping_clq.html#mapping",
    "title": "Mapping and LCLQ",
    "section": "Mapping",
    "text": "Mapping\nHDB locations and relevant information\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\n  tm_shape(resale_sf) +\n  tm_dots(col = \"resale_price\", \n          id = \"full_address\", # bold in popup\n          popup.vars = c(\"Resale Price:\" = \"resale_price\",\n                         \"Flat Type:\" = \"flat_type\", \n                         \"Flat Model:\" = \"flat_model\",\n                         \"Floor Area (sqm):\" = \"floor_area_sqm\",\n                         \"Remaining Lease:\" = \"remaining_lease\"\n                         ),\n          title = \"Resale Prices\")\n\nHDB and Supermarket\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\n  tm_shape(resale_sf) +\n  tm_dots(col = \"resale_price\", \n          id = \"full_address\", # bold in popup\n          popup.vars = c(\"Resale Price:\" = \"resale_price\",\n                         \"Flat Type:\" = \"flat_type\", \n                         \"Flat Model:\" = \"flat_model\",\n                         \"Floor Area (sqm):\" = \"floor_area_sqm\",\n                         \"Remaining Lease:\" = \"remaining_lease\"\n                         ),\n          title = \"Resale Prices\") +\n  tm_shape(supermarket_sf) +\n  tm_dots(alpha=0.5,\n        col=\"#FF0000\",\n        size=0.05) +\n  tm_view(set.zoom.limits = c(10, 16))\n\nHDB and Kindergartens\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\ntm_shape(resale_sf_address) +\n  tm_dots(alpha=0.5, #affects transparency of points\n          col=\"#FCE883\",\n          size=0.05)+\ntm_shape(kindergartens_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#FFD1DC\",\n          size=0.05)\n\nEarly Childhood/Primary Schools\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.5, #affects transparency of points\n          col=\"#FCE883\",\n          size=0.05)+\ntm_shape(kindergartens_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#FFD1DC\",\n          size=0.05) +\ntm_shape(primary_school_sf) +\n  tm_dots(alpha=0.5,\n        col=\"#FF0000\",\n        size=0.05) +\n  tm_view(set.zoom.limits = c(10, 16))\n\nTransport (Bus)\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\ntm_shape(bus_stop_sf) +\n  tm_dots(alpha=0.5, #affects transparency of points\n          col=\"#007FFF\",\n          size=0.05)\n  tm_view(set.zoom.limits = c(10, 14))\n\nTransport(Train)\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) + \ntm_shape(mrt_sf)+ \n  tm_dots(col = \"red\",\n             size = 0.01,\n             border.col = \"black\",\n             border.lwd = 0.5,\n           legend.show = FALSE) +\n  tm_view(set.zoom.limits = c(10, 16))\n\nHDB and another amenity function\n\ntmap_mode(\"view\")\ntm_shape(mpsz_sf) +\n  tm_polygons(\"REGION_N\",\n              alpha = 0.2) +\n  tm_shape(resale_sf) +\n  tm_dots(col = \"resale_price\", \n          id = \"full_address\", # bold in popup\n          popup.vars = c(\"Resale Price:\" = \"resale_price\",\n                         \"Flat Type:\" = \"flat_type\", \n                         \"Flat Model:\" = \"flat_model\",\n                         \"Floor Area (sqm):\" = \"floor_area_sqm\",\n                         \"Remaining Lease:\" = \"remaining_lease\"\n                         ),\n          title = \"Resale Prices\") +\n  tm_shape(supermarket_sf) +\n  tm_dots(alpha=0.5,\n        col=\"#FF0000\",\n        size=0.05) +\n  tm_view(set.zoom.limits = c(10, 16))"
  },
  {
    "objectID": "code_draft/mapping_clq.html#lclq",
    "href": "code_draft/mapping_clq.html#lclq",
    "title": "Mapping and LCLQ",
    "section": "LCLQ",
    "text": "LCLQ\n\n#HDB locations\nhdb_a <- resale_sf %>%\n  select(geometry) %>% \n  mutate(geometry= geometry,\n         type = \"HDB\") \n\n\n#formula to get the LCLQ data\n\n#hdb_a, amenity df, amenity name\ncql_func <- function(df1,df2,nameB){\n  df2_b <- df2 %>%\n  select(geometry) %>% \n  mutate(geometry= geometry,\n         type = nameB) \n  combined <- rbindlist(list(df1, df2_b),use.names=FALSE)\n  combined_sf <- st_as_sf(combined) %>%\n  st_transform(crs = 3414)\n  \n  nb <- include_self(\n  st_knn(st_geometry(combined_sf), 6))\n  \n  wt <- st_kernel_weights(nb, \n                        combined_sf, \n                        \"gaussian\", \n                        adaptive = TRUE)\n  \n  hdb_locations <- combined_sf %>%\n  filter(type == \"HDB\")\n  A <- hdb_locations$type\n  \n  df2_locations <- combined_sf %>%\n  filter(type == nameB)\n  \n  B <- df2_locations$type\n  \n  LCLQ <- local_colocation(A, B, wt, nb, 99) # 100 simulations\n  LCLQ_stores <- cbind(combined_sf, LCLQ)\n  \n  write_rds(LCLQ_stores, paste0(\"data/rds/clq/hdb_\",nameB,\".rds\"))\n  }\n\n\n# dont run again\ncql_func(hdb_a,childcare_sf,\"Childcare\")\ncql_func(hdb_a,eldercare_sf,\"Eldercare\")\ncql_func(hdb_a,kindergartens_sf,\"Kindegarten\")\ncql_func(hdb_a,nationalparks_sf,\"NationalParks\")\ncql_func(hdb_a,gyms_sf,\"Gym\")\ncql_func(hdb_a,pharmacy_sf,\"Pharmacy\")\ncql_func(hdb_a,supermarket_sf,\"Supermarket\")\ncql_func(hdb_a,bus_stop_sf,\"Bus\")\ncql_func(hdb_a,HDB_carpark_sf,\"Carparks\")\ncql_func(hdb_a,mrt_sf,\"Mrt\")\ncql_func(hdb_a,primary_school_sf,\"PrimarySchool\")\ncql_func(hdb_a,hawkercentre_new_sf,\"Hawker\")\ncql_func(hdb_a,shopping_mall_sf,\"ShoppingMall\")"
  },
  {
    "objectID": "code_draft/mapping_clq.html#import-lclq-data",
    "href": "code_draft/mapping_clq.html#import-lclq-data",
    "title": "Mapping and LCLQ",
    "section": "Import LCLQ Data",
    "text": "Import LCLQ Data\n\n# import the clq files\nchildcare_clq<- readRDS(\"data/rds/clq/hdb_Childcare.rds\")\neldercare_clq<- readRDS(\"data/rds/clq/hdb_Eldercare.rds\")\nkindergarten_clq<- readRDS(\"data/rds/clq/hdb_Kindegarten.rds\")\nnationalParks_clq<- readRDS(\"data/rds/clq/hdb_NationalParks.rds\")\ngym_clq<- readRDS(\"data/rds/clq/hdb_Gym.rds\")\npharmacy_clq<- readRDS(\"data/rds/clq/hdb_Pharmacy.rds\")\nsupermarket_clq<- readRDS(\"data/rds/clq/hdb_Supermarket.rds\")\nbus_clq<- readRDS(\"data/rds/clq/hdb_Bus.rds\")\ncarpark_clq<- readRDS(\"data/rds/clq/hdb_Carparks.rds\")\nmrt_clq<- readRDS(\"data/rds/clq/hdb_Mrt.rds\")\nprimarySchool_clq<- readRDS(\"data/rds/clq/hdb_PrimarySchool.rds\")\nhawker_clq<- readRDS(\"data/rds/clq/hdb_Hawker.rds\")\nshoppingMall_clq<- readRDS(\"data/rds/clq/hdb_ShoppingMall.rds\")\n\n#\nmpsz_sf<-readRDS(\"data/rds/mpsz_sf.rds\")\n\n\n#check pvalues\n\n unique(st_drop_geometry(childcare_clq)[, 3])\n unique(st_drop_geometry(eldercare_clq)[, 3])\n unique(st_drop_geometry(kindergarten_clq)[, 3])\n unique(st_drop_geometry(nationalParks_clq)[, 3])\n unique(st_drop_geometry(gym_clq)[, 3])\n unique(st_drop_geometry(pharmacy_clq)[, 3])\n unique(st_drop_geometry(supermarket_clq)[, 3])\n unique(st_drop_geometry(bus_clq)[, 3]) # has p-value above 0.05\n unique(st_drop_geometry(carpark_clq)[, 3])\n unique(st_drop_geometry(mrt_clq)[, 3])\n unique(st_drop_geometry(primarySchool_clq)[, 3])\n unique(st_drop_geometry(hawker_clq)[, 3])\n unique(st_drop_geometry(shoppingMall_clq)[, 3])\n\n\n#intepretation reference: https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/learnmorecolocationanalysis.htm\n\n# filtered_data <- subset(eldercare_clq, p_sim_Eldercare < 0.05)\n\namenity_name <- names(shoppingMall_clq)[2]\namenity_pvalue <- names(shoppingMall_clq)[3]\n\ntmap_mode(\"plot\")\ntm_shape(mpsz_sf) +\n  tm_polygons() +\ntm_shape(shoppingMall_clq, subset = shoppingMall_clq$amenity_pvalue < 0.05) + #filter out p_value less than 0.05 \n  tm_symbols(col = amenity_name,\n             size = 0.3,\n             border.col = \"black\",\n             border.lwd = 0.5)  \n#+\n#        tm_view(set.zoom.limits = c(10, 14))\n\n\nplot_clq <- function(a_name) {\n    result <- readRDS(paste0(\"data/rds/clq/hdb_\",a_name, \".rds\"))\n    \n    amenity_name <- names(result)[2]\n    amenity_pvalue <- names(result)[3]\n    tmap_mode(\"plot\")\n    clq_map <-\n      tm_shape(mpsz_sf) +\n      tm_polygons() +\n      tm_shape(result, subset = result$amenity_pvalue < 0.05) + #filter out p_value less than 0.05 \n      tm_dots(col = amenity_name,\n              size = 0.01,\n              border.col = \"black\",\n              border.lwd = 0.5)\n    \n}\n\n\nplot_clq(\"Bus\")"
  }
]